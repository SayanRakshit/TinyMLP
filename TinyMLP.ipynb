{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqJd4eLE7SV_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "try:\n",
        "    drive.flush_and_unmount()\n",
        "except Exception:\n",
        "    pass\n",
        "import shutil, os, time\n",
        "shutil.rmtree(\"/content/drive\", ignore_errors=True)\n",
        "time.sleep(1)\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/code_data25/Basu"
      ],
      "metadata": {
        "id": "ObkzEGBB7brM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== MLP with per-class 80/20 split (train/test), CFO/RSS skipped =====\n",
        "!pip -q install h5py scikit-learn\n",
        "\n",
        "import h5py, numpy as np, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "H5_PATH = \"Dataset-RFFI/dataset_training_no_aug.h5\"   # <-- set your path\n",
        "BATCH = 512\n",
        "EPOCHS = 1000\n",
        "LR = 1e-3\n",
        "HIDDEN = 512\n",
        "STANDARDIZE = True   # per-feature mean/std on /data\n",
        "RNG_SEED = 42        # reproducible split\n",
        "\n",
        "# -------- Dataset: only /data and /label --------\n",
        "class WifiH5NoMeta(Dataset):\n",
        "    def __init__(self, path, standardize=True):\n",
        "        self.f = h5py.File(path, \"r\")\n",
        "        self.X = self.f[\"/data\"]                         # (N, D) float64\n",
        "        y_raw = np.asarray(self.f[\"/label\"][0], np.int64)\n",
        "        uniq = np.unique(y_raw); self.label_map = {int(v): i for i,v in enumerate(uniq)}\n",
        "        self.y = np.array([self.label_map[int(v)] for v in y_raw], np.int64)\n",
        "\n",
        "        self.mu = self.sd = None\n",
        "        if standardize:\n",
        "            # chunked mean/std so we don't load all into RAM\n",
        "            n, d = self.X.shape\n",
        "            m = np.zeros(d, np.float64); v = np.zeros(d, np.float64); cnt = 0\n",
        "            bs = 512\n",
        "            for i in range(0, n, bs):\n",
        "                chunk = self.X[i:i+bs].astype(np.float64)\n",
        "                m_c, v_c = chunk.mean(axis=0), chunk.var(axis=0)\n",
        "                if cnt == 0:\n",
        "                    m[:] = m_c; v[:] = v_c * chunk.shape[0]; cnt = chunk.shape[0]\n",
        "                else:\n",
        "                    n2 = cnt + chunk.shape[0]; delta = m_c - m\n",
        "                    m += delta * (chunk.shape[0] / n2)\n",
        "                    v += v_c * chunk.shape[0] + (delta**2) * (cnt * chunk.shape[0] / n2)\n",
        "                    cnt = n2\n",
        "            var = v / max(cnt-1,1)\n",
        "            self.mu = torch.from_numpy(m.astype(np.float32))\n",
        "            self.sd = torch.from_numpy(np.sqrt(var + 1e-8).astype(np.float32))\n",
        "\n",
        "    def __len__(self): return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.X[idx].astype(np.float32))   # (D,)\n",
        "        if self.mu is not None: x = (x - self.mu) / self.sd\n",
        "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "# -------- Build per-class 80/20 split (train/test) --------\n",
        "full = WifiH5NoMeta(H5_PATH, standardize=STANDARDIZE)\n",
        "N, D = full.X.shape\n",
        "num_classes = len(full.label_map)\n",
        "print(f\"N={N}, D={D}, classes={num_classes}\")\n",
        "print(\"label_map:\", full.label_map)\n",
        "\n",
        "labels = np.array(full.y)\n",
        "rng = np.random.default_rng(RNG_SEED)\n",
        "\n",
        "train_idx_list, test_idx_list = [], []\n",
        "for c in np.unique(labels):\n",
        "    idx = np.where(labels == c)[0]\n",
        "    rng.shuffle(idx)\n",
        "    n_c = len(idx)\n",
        "    n_train = int(np.floor(0.8 * n_c))\n",
        "    # ensure both splits non-empty when possible\n",
        "    if n_train == 0 and n_c >= 2: n_train = 1\n",
        "    if n_c - n_train == 0 and n_c >= 2: n_train = n_c - 1\n",
        "    train_idx_list.append(idx[:n_train])\n",
        "    test_idx_list.append(idx[n_train:])\n",
        "\n",
        "train_idx = np.concatenate(train_idx_list)\n",
        "test_idx  = np.concatenate(test_idx_list)\n",
        "\n",
        "# Show per-class counts for sanity\n",
        "print(\"\\nPer-class counts (train/test):\")\n",
        "for c in np.unique(labels):\n",
        "    tr = np.sum(labels[train_idx] == c)\n",
        "    te = np.sum(labels[test_idx]  == c)\n",
        "    print(f\"class {c}: train={tr}, test={te}, total={tr+te}\")\n",
        "\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "train_ds, test_ds = Subset(full, train_idx), Subset(full, test_idx)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# -------- Simple MLP: Linear -> ReLU -> Linear --------\n",
        "class TinyMLP(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes, hidden=HIDDEN):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, num_classes)\n",
        "        self.act = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.act(self.fc1(x)))\n",
        "\n",
        "model = TinyMLP(D, num_classes).to(DEVICE)\n",
        "\n",
        "cls, cnt = np.unique(labels[train_idx], return_counts=True)\n",
        "weights = torch.tensor(cnt.mean() / cnt, dtype=torch.float32, device=DEVICE)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "# -------- Train / Test --------\n",
        "def run_epoch(loader, train=True):\n",
        "    (model.train() if train else model.eval())\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    with torch.set_grad_enabled(train):\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            if train: optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            loss_sum += loss.item() * yb.size(0)\n",
        "            correct += (logits.argmax(1) == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    return loss_sum/total, correct/max(1,total)\n",
        "\n",
        "\n",
        "best_acc = 0.0\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader, True)\n",
        "    te_loss, te_acc = run_epoch(test_loader,  False)\n",
        "    print(f\"Epoch {ep:02d} | train {tr_loss:.4f}/{tr_acc:.4f} | test {te_loss:.4f}/{te_acc:.4f}\")\n",
        "    if te_acc > best_acc:\n",
        "        best_acc = te_acc\n",
        "        torch.save({\n",
        "            \"model\": model.state_dict(),\n",
        "            \"label_map\": full.label_map,\n",
        "            \"in_dim\": D,\n",
        "            \"num_classes\": num_classes\n",
        "        }, \"/content/drive/MyDrive/code_data25/Basu/model_save/best_router_mlp_80_20.pth\")\n",
        "\n",
        "# -------- Final report (on test set) --------\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        pr = model(xb.to(DEVICE)).argmax(1).cpu().numpy()\n",
        "        y_pred.append(pr); y_true.append(yb.numpy())\n",
        "y_true = np.concatenate(y_true); y_pred = np.concatenate(y_pred)\n",
        "\n",
        "print(\"\\nTest accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"\\nPer-class report:\\n\", classification_report(y_true, y_pred, digits=4))\n",
        "print(\"\\nConfusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "SwmRGbrt8b5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, json\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# === load your trained model weights ===\n",
        "CKPT_PATH = \"/content/drive/MyDrive/code_data25/Basu/model_save/best_router_mlp_80_20.pth\"  # <-- change if needed\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# If you saved a dict with {\"model\": state_dict, ...}\n",
        "ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
        "state_dict = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
        "\n",
        "model.to(DEVICE)\n",
        "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "print(\"Loaded weights. Missing keys:\", missing, \"| Unexpected keys:\", unexpected)\n",
        "\n",
        "# write label_map if present\n",
        "if isinstance(ckpt, dict) and \"label_map\" in ckpt:\n",
        "    with open(\"label_map.json\", \"w\") as f:\n",
        "        json.dump(ckpt[\"label_map\"], f, indent=2)\n",
        "\n",
        "# === evaluation + saving ===\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        pr = model(xb.to(DEVICE)).argmax(1).cpu().numpy()\n",
        "        y_pred.append(pr); y_true.append(yb.numpy())\n",
        "y_true = np.concatenate(y_true); y_pred = np.concatenate(y_pred)\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "rep = classification_report(y_true, y_pred, digits=4)\n",
        "cm  = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/code_data25/Basu/save_results/test_accuracy.txt\", \"w\") as f:\n",
        "    f.write(f\"{acc:.6f}\\n\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/code_data25/Basu/save_results/classification_report.txt\", \"w\") as f:\n",
        "    f.write(rep + \"\\n\")\n",
        "\n",
        "np.savetxt(\"/content/drive/MyDrive/code_data25/Basu/save_results/confusion_matrix.txt\", cm, fmt=\"%d\")\n",
        "\n",
        "np.savetxt(\n",
        "    \"/content/drive/MyDrive/code_data25/Basu/save_results/predictions.csv\",\n",
        "    np.column_stack([y_true, y_pred]),\n",
        "    fmt=\"%d\",\n",
        "    delimiter=\",\",\n",
        "    header=\"y_true,y_pred\",\n",
        "    comments=\"\"\n",
        ")\n",
        "\n",
        "df = pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred})\n",
        "per_class_acc = df.groupby(\"y_true\").apply(lambda g: float((g.y_true == g.y_pred).mean()))\n",
        "per_class_acc.to_csv(\"/content/drive/MyDrive/code_data25/Basu/save_results/per_class_accuracy.csv\", header=[\"accuracy\"])\n",
        "\n",
        "print(\"Saved: test_accuracy.txt, classification_report.txt, confusion_matrix.txt, predictions.csv, per_class_accuracy.csv\")\n",
        "\n",
        "# ========= single-sample latency + GFLOPs report =========\n",
        "import os, time, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/code_data25/Basu/save_results\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# 1) Take one real sample from your test loader\n",
        "model.eval()\n",
        "xb_one, _ = next(iter(test_loader))          # uses your real preprocessing pipeline\n",
        "xb_one = xb_one[:1].to(DEVICE)               # shape [1, D]\n",
        "\n",
        "# 2) Accurate timing (GPU-aware)\n",
        "def time_inference(model, x, warmup=30, repeats=200):\n",
        "    with torch.no_grad():\n",
        "        # warmup (not measured)\n",
        "        for _ in range(warmup):\n",
        "            _ = model(x)\n",
        "            if x.is_cuda:\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "        times = []\n",
        "        if x.is_cuda:\n",
        "            start = torch.cuda.Event(enable_timing=True)\n",
        "            end   = torch.cuda.Event(enable_timing=True)\n",
        "            for _ in range(repeats):\n",
        "                start.record(); _ = model(x); end.record()\n",
        "                torch.cuda.synchronize()\n",
        "                times.append(start.elapsed_time(end) / 1000.0)  # seconds\n",
        "        else:\n",
        "            for _ in range(repeats):\n",
        "                t0 = time.perf_counter(); _ = model(x); t1 = time.perf_counter()\n",
        "                times.append(t1 - t0)\n",
        "\n",
        "    times = np.array(times, dtype=np.float64)\n",
        "    return {\n",
        "        \"device\": \"cuda\" if x.is_cuda else \"cpu\",\n",
        "        \"batch_size\": int(x.shape[0]),\n",
        "        \"runs\": int(repeats),\n",
        "        \"mean_s\": float(times.mean()),\n",
        "        \"median_s\": float(np.median(times)),\n",
        "        \"p90_s\": float(np.percentile(times, 90)),\n",
        "        \"p95_s\": float(np.percentile(times, 95)),\n",
        "        \"min_s\": float(times.min()),\n",
        "        \"max_s\": float(times.max()),\n",
        "    }\n",
        "\n",
        "# 3) FLOPs for MLP (sum over Linear layers): 1 multiply + 1 add = 2 FLOPs\n",
        "def mlp_flops_per_sample(model):\n",
        "    flops = 0\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            flops += 2 * m.in_features * m.out_features\n",
        "    return flops  # per-sample\n",
        "\n",
        "def num_params(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "timing = time_inference(model, xb_one, warmup=30, repeats=200)\n",
        "flops  = mlp_flops_per_sample(model)\n",
        "report = {\n",
        "    \"device\": timing[\"device\"],\n",
        "    \"batch_size\": timing[\"batch_size\"],\n",
        "    \"runs\": timing[\"runs\"],\n",
        "    \"per_sample_time_s_mean\": timing[\"mean_s\"],\n",
        "    \"per_sample_time_s_median\": timing[\"median_s\"],\n",
        "    \"per_sample_time_s_p90\": timing[\"p90_s\"],\n",
        "    \"per_sample_time_s_p95\": timing[\"p95_s\"],\n",
        "    \"per_sample_time_s_min\": timing[\"min_s\"],\n",
        "    \"throughput_samples_per_s_mean\": (1.0 / timing[\"mean_s\"]) if timing[\"mean_s\"] > 0 else None,\n",
        "    \"params_total\": int(num_params(model)),\n",
        "    \"flops_per_sample\": int(flops),\n",
        "    \"gflops_per_sample\": float(flops / 1e9),\n",
        "}\n",
        "\n",
        "print(json.dumps(report, indent=2))\n",
        "Path(f\"{OUT_DIR}/inference_profile.json\").write_text(json.dumps(report, indent=2))\n",
        "print(f\"Saved {OUT_DIR}/inference_profile.json\")\n",
        "\n",
        "# ========= per-class bar, ROC curves, and t-SNE (save PNGs) =========\n",
        "import os, json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/code_data25/Basu/save_results\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- A) Per-class accuracy bar plot ----------\n",
        "df_eval = pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred})\n",
        "per_class_acc = df_eval.groupby(\"y_true\").apply(lambda g: float((g.y_true == g.y_pred).mean()))\n",
        "per_class_acc = per_class_acc.sort_values(ascending=True)  # ascending so worst classes at top\n",
        "\n",
        "plt.figure(figsize=(7, max(4, 0.22 * len(per_class_acc))), dpi=400)\n",
        "plt.barh(per_class_acc.index.astype(str), per_class_acc.values)\n",
        "plt.xlabel(\"Per-class Accuracy\")\n",
        "plt.ylabel(\"Class ID\")\n",
        "plt.title(\"Per-class Accuracy (Sorted)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUT_DIR}/per_class_accuracy_bar.png\", bbox_inches=\"tight\", facecolor=\"white\")\n",
        "plt.close()\n",
        "\n",
        "# ---------- B) ROC curves (micro/macro + OvR) ----------\n",
        "# We need class probabilities.\n",
        "model.eval()\n",
        "all_scores, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        logits = model(xb.to(DEVICE))\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "        all_scores.append(probs)\n",
        "        all_labels.append(yb.numpy())\n",
        "y_score = np.vstack(all_scores)  # shape [N, C]\n",
        "y_true_arr = np.concatenate(all_labels)  # shape [N]\n",
        "classes = np.unique(y_true_arr)\n",
        "n_classes = classes.size\n",
        "\n",
        "# Binarize labels for multi-class ROC\n",
        "y_true_bin = label_binarize(y_true_arr, classes=classes)  # shape [N, C]\n",
        "\n",
        "# Compute micro, macro, and OvR ROC\n",
        "fpr, tpr, roc_auc = {}, {}, {}\n",
        "# OvR for each class (columns aligned with 'classes')\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# micro-average\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# macro-average\n",
        "# compute average of the interpolated TPRs\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i], left=0, right=1)\n",
        "mean_tpr /= n_classes\n",
        "fpr[\"macro\"], tpr[\"macro\"] = all_fpr, mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Plot\n",
        "fig = plt.figure(figsize=(7, 6), dpi=400, facecolor=\"white\")\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1)\n",
        "\n",
        "# Emphasize micro & macro\n",
        "ax.plot(fpr[\"micro\"], tpr[\"micro\"], linewidth=2, label=f\"micro-average (AUC = {roc_auc['micro']:.3f})\")\n",
        "ax.plot(fpr[\"macro\"], tpr[\"macro\"], linewidth=2, label=f\"macro-average (AUC = {roc_auc['macro']:.3f})\")\n",
        "\n",
        "# Plot a few one-vs-rest class curves (to avoid clutter if many classes)\n",
        "MAX_OVR = 8\n",
        "pick = np.linspace(0, n_classes-1, min(n_classes, MAX_OVR), dtype=int)\n",
        "for i in pick:\n",
        "    ax.plot(fpr[i], tpr[i], linewidth=1, label=f\"class {classes[i]} (AUC = {roc_auc[i]:.3f})\")\n",
        "\n",
        "ax.set_xlabel(\"False Positive Rate\")\n",
        "ax.set_ylabel(\"True Positive Rate\")\n",
        "ax.set_title(\"ROC Curves: micro, macro, and One-vs-Rest\")\n",
        "ax.legend(loc=\"lower right\", fontsize=7)\n",
        "fig.tight_layout()\n",
        "fig.savefig(f\"{OUT_DIR}/roc_curves_micro_macro_onevsrest.png\", bbox_inches=\"tight\", facecolor=\"white\")\n",
        "plt.close(fig)\n",
        "\n",
        "# ---------- C) t-SNE on penultimate features (fallback to logits if needed) ----------\n",
        "# We’ll try to grab the penultimate activations by registering a forward hook on the last Linear layer’s input.\n",
        "# If that fails, we’ll use logits as features.\n",
        "last_linear = None\n",
        "for m in model.modules():\n",
        "    if isinstance(m, nn.Linear):\n",
        "        last_linear = m\n",
        "# Container to hold features from the forward hook\n",
        "_feats = []\n",
        "def hook_fn(module, inputs, output):\n",
        "    # inputs is a tuple; we want the input activations to this linear layer (penultimate features)\n",
        "    _feats.append(inputs[0].detach().cpu())\n",
        "\n",
        "h = None\n",
        "if last_linear is not None:\n",
        "    h = last_linear.register_forward_hook(hook_fn)\n",
        "\n",
        "# Run forward passes to collect features\n",
        "model.eval()\n",
        "feat_list, lab_list = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        _ = model(xb.to(DEVICE))\n",
        "        if _feats:\n",
        "            feat_list.append(_feats[-1])  # latest batch’s features\n",
        "        else:\n",
        "            # Fallback to logits if hook didn't fire\n",
        "            feat_list.append(_.detach().cpu())\n",
        "        lab_list.append(yb)\n",
        "\n",
        "# Cleanup hook\n",
        "if h is not None:\n",
        "    h.remove()\n",
        "\n",
        "feats = torch.cat(feat_list, dim=0).cpu().numpy()\n",
        "labs  = torch.cat(lab_list, dim=0).cpu().numpy()\n",
        "\n",
        "# To keep t-SNE stable and fast, optionally subsample if huge\n",
        "MAX_TSNE = 5000\n",
        "if feats.shape[0] > MAX_TSNE:\n",
        "    rng = np.random.default_rng(42)\n",
        "    idx = rng.choice(feats.shape[0], size=MAX_TSNE, replace=False)\n",
        "    feats_vis = feats[idx]\n",
        "    labs_vis  = labs[idx]\n",
        "else:\n",
        "    feats_vis, labs_vis = feats, labs\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\", init=\"pca\", random_state=42)\n",
        "emb  = tsne.fit_transform(feats_vis)\n",
        "\n",
        "plt.figure(figsize=(7, 6), dpi=400)\n",
        "sc = plt.scatter(emb[:,0], emb[:,1], c=labs_vis, s=5, alpha=0.8)\n",
        "plt.title(\"t-SNE of Learned Representations (penultimate features)\")\n",
        "plt.xlabel(\"t-SNE dim 1\"); plt.ylabel(\"t-SNE dim 2\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUT_DIR}/tsne_features.png\", bbox_inches=\"tight\", facecolor=\"white\")\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved:\",\n",
        "      f\"{OUT_DIR}/per_class_accuracy_bar.png,\",\n",
        "      f\"{OUT_DIR}/roc_curves_micro_macro_onevsrest.png,\",\n",
        "      f\"{OUT_DIR}/tsne_features.png\")\n",
        "# ========= END ADD =========\n",
        "\n",
        "\n",
        "\n",
        "import json, os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# class names from label_map\n",
        "class_names = None\n",
        "if os.path.exists(\"label_map.json\"):\n",
        "    with open(\"label_map.json\") as f:\n",
        "        lm = json.load(f)\n",
        "    inv = {v: k for k, v in lm.items()}\n",
        "    class_names = [inv[i] for i in range(len(inv))]\n",
        "else:\n",
        "    n_classes = cm.shape[0]\n",
        "    class_names = [str(i) for i in range(n_classes)]\n",
        "\n",
        "def _annotated_confmat(M, title, outfile, fmt=\"d\", normalize=False):\n",
        "    \"\"\"Save a confusion matrix heatmap (PNG, high DPI) with a white background.\"\"\"\n",
        "    fig_w = max(6, min(0.35 * M.shape[1], 20))\n",
        "    fig_h = max(5, min(0.35 * M.shape[0], 20))\n",
        "    fig = plt.figure(figsize=(fig_w, fig_h), dpi=600, facecolor=\"white\")\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_facecolor(\"white\")\n",
        "\n",
        "    # Normalization if requested\n",
        "    if normalize:\n",
        "        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
        "            row_sums = M.sum(axis=1, keepdims=True)\n",
        "            M_plot = np.divide(M, row_sums, out=np.zeros_like(M, dtype=float), where=row_sums!=0) * 100.0\n",
        "    else:\n",
        "        M_plot = M\n",
        "\n",
        "    # visually clean colormap (e.g., viridis, cividis, or blues)\n",
        "    im = ax.imshow(M_plot, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    cbar.ax.set_ylabel(\"%\" if normalize else \"Count\", rotation=90, va=\"center\", fontsize=8)\n",
        "\n",
        "    ax.set_title(title, pad=12, fontsize=10)\n",
        "    ax.set_xlabel(\"Predicted\", fontsize=9)\n",
        "    ax.set_ylabel(\"True\", fontsize=9)\n",
        "\n",
        "    tick_idx = np.arange(M.shape[0])\n",
        "    max_ticks = 50\n",
        "    if len(class_names) <= max_ticks:\n",
        "        ax.set_xticks(tick_idx)\n",
        "        ax.set_xticklabels(class_names, rotation=90, fontsize=7)\n",
        "        ax.set_yticks(tick_idx)\n",
        "        ax.set_yticklabels(class_names, fontsize=7)\n",
        "    else:\n",
        "        ax.set_xticks(tick_idx)\n",
        "        ax.set_xticklabels(tick_idx, rotation=90, fontsize=7)\n",
        "        ax.set_yticks(tick_idx)\n",
        "        ax.set_yticklabels(tick_idx, fontsize=7)\n",
        "\n",
        "    # Annotate cells if matrix is not too large\n",
        "    if M.shape[0] <= 50:\n",
        "        display = M_plot if normalize else M.astype(int)\n",
        "        fmt_str = \"{:.1f}\" if normalize else \"{:d}\"\n",
        "        thresh = np.nanmax(M_plot) * 0.5 if np.isfinite(np.nanmax(M_plot)) else 0.0\n",
        "        for i in range(M.shape[0]):\n",
        "            for j in range(M.shape[1]):\n",
        "                val = display[i, j]\n",
        "                color = \"white\" if (np.isfinite(M_plot[i, j]) and M_plot[i, j] > thresh) else \"black\"\n",
        "                ax.text(j, i, fmt_str.format(val),\n",
        "                        ha=\"center\", va=\"center\", color=color, fontsize=6)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(outfile, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "    plt.close(fig)\n",
        "\n",
        "# Save both versions (white background)\n",
        "_annotated_confmat(cm, \"Confusion Matrix (Counts)\",\n",
        "                   \"/content/drive/MyDrive/code_data25/Basu/save_results/confusion_matrix_counts.png\",\n",
        "                   fmt=\"d\", normalize=False)\n",
        "_annotated_confmat(cm, \"Confusion Matrix (Row-Normalized %)\",\n",
        "                   \"/content/drive/MyDrive/code_data25/Basu/save_results/confusion_matrix_normalized.png\",\n",
        "                   fmt=\".1f\", normalize=True)\n",
        "\n",
        "print(\"Saved white-background PNGs: confusion_matrix_counts.png, confusion_matrix_normalized.png\")\n"
      ],
      "metadata": {
        "id": "tl1HlU949csO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qs2ub5xNE1g6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}