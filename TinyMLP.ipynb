{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqJd4eLE7SV_",
        "outputId": "ace8c8d5-446b-43c0-8585-66650c19f9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "try:\n",
        "    drive.flush_and_unmount()\n",
        "except Exception:\n",
        "    pass\n",
        "import shutil, os, time\n",
        "shutil.rmtree(\"/content/drive\", ignore_errors=True)\n",
        "time.sleep(1)\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/code_data25/Basu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObkzEGBB7brM",
        "outputId": "c25059b8-4762-4a85-c98b-9316a0be5691"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/code_data25/Basu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== MLP with per-class 80/20 split (train/test), CFO/RSS skipped =====\n",
        "!pip -q install h5py scikit-learn\n",
        "\n",
        "import h5py, numpy as np, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "H5_PATH = \"Dataset-RFFI/dataset_training_no_aug.h5\"   # <-- set your path\n",
        "BATCH = 512\n",
        "EPOCHS = 1000\n",
        "LR = 1e-3\n",
        "HIDDEN = 512\n",
        "STANDARDIZE = True   # per-feature mean/std on /data\n",
        "RNG_SEED = 42        # reproducible split\n",
        "\n",
        "# -------- Dataset: only /data and /label --------\n",
        "class WifiH5NoMeta(Dataset):\n",
        "    def __init__(self, path, standardize=True):\n",
        "        self.f = h5py.File(path, \"r\")\n",
        "        self.X = self.f[\"/data\"]                         # (N, D) float64\n",
        "        y_raw = np.asarray(self.f[\"/label\"][0], np.int64)\n",
        "        uniq = np.unique(y_raw); self.label_map = {int(v): i for i,v in enumerate(uniq)}\n",
        "        self.y = np.array([self.label_map[int(v)] for v in y_raw], np.int64)\n",
        "\n",
        "        self.mu = self.sd = None\n",
        "        if standardize:\n",
        "            # chunked mean/std so we don't load all into RAM\n",
        "            n, d = self.X.shape\n",
        "            m = np.zeros(d, np.float64); v = np.zeros(d, np.float64); cnt = 0\n",
        "            bs = 512\n",
        "            for i in range(0, n, bs):\n",
        "                chunk = self.X[i:i+bs].astype(np.float64)\n",
        "                m_c, v_c = chunk.mean(axis=0), chunk.var(axis=0)\n",
        "                if cnt == 0:\n",
        "                    m[:] = m_c; v[:] = v_c * chunk.shape[0]; cnt = chunk.shape[0]\n",
        "                else:\n",
        "                    n2 = cnt + chunk.shape[0]; delta = m_c - m\n",
        "                    m += delta * (chunk.shape[0] / n2)\n",
        "                    v += v_c * chunk.shape[0] + (delta**2) * (cnt * chunk.shape[0] / n2)\n",
        "                    cnt = n2\n",
        "            var = v / max(cnt-1,1)\n",
        "            self.mu = torch.from_numpy(m.astype(np.float32))\n",
        "            self.sd = torch.from_numpy(np.sqrt(var + 1e-8).astype(np.float32))\n",
        "\n",
        "    def __len__(self): return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.X[idx].astype(np.float32))   # (D,)\n",
        "        if self.mu is not None: x = (x - self.mu) / self.sd\n",
        "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "# -------- Build per-class 80/20 split (train/test) --------\n",
        "full = WifiH5NoMeta(H5_PATH, standardize=STANDARDIZE)\n",
        "N, D = full.X.shape\n",
        "num_classes = len(full.label_map)\n",
        "print(f\"N={N}, D={D}, classes={num_classes}\")\n",
        "print(\"label_map:\", full.label_map)\n",
        "\n",
        "labels = np.array(full.y)\n",
        "rng = np.random.default_rng(RNG_SEED)\n",
        "\n",
        "train_idx_list, test_idx_list = [], []\n",
        "for c in np.unique(labels):\n",
        "    idx = np.where(labels == c)[0]\n",
        "    rng.shuffle(idx)\n",
        "    n_c = len(idx)\n",
        "    n_train = int(np.floor(0.8 * n_c))\n",
        "    # ensure both splits non-empty when possible\n",
        "    if n_train == 0 and n_c >= 2: n_train = 1\n",
        "    if n_c - n_train == 0 and n_c >= 2: n_train = n_c - 1\n",
        "    train_idx_list.append(idx[:n_train])\n",
        "    test_idx_list.append(idx[n_train:])\n",
        "\n",
        "train_idx = np.concatenate(train_idx_list)\n",
        "test_idx  = np.concatenate(test_idx_list)\n",
        "\n",
        "# Show per-class counts for sanity\n",
        "print(\"\\nPer-class counts (train/test):\")\n",
        "for c in np.unique(labels):\n",
        "    tr = np.sum(labels[train_idx] == c)\n",
        "    te = np.sum(labels[test_idx]  == c)\n",
        "    print(f\"class {c}: train={tr}, test={te}, total={tr+te}\")\n",
        "\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "train_ds, test_ds = Subset(full, train_idx), Subset(full, test_idx)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# -------- Simple MLP: Linear -> ReLU -> Linear --------\n",
        "class TinyMLP(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes, hidden=HIDDEN):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, num_classes)\n",
        "        self.act = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.act(self.fc1(x)))\n",
        "\n",
        "model = TinyMLP(D, num_classes).to(DEVICE)\n",
        "\n",
        "cls, cnt = np.unique(labels[train_idx], return_counts=True)\n",
        "weights = torch.tensor(cnt.mean() / cnt, dtype=torch.float32, device=DEVICE)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "# -------- Train / Test --------\n",
        "def run_epoch(loader, train=True):\n",
        "    (model.train() if train else model.eval())\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    with torch.set_grad_enabled(train):\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            if train: optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            loss_sum += loss.item() * yb.size(0)\n",
        "            correct += (logits.argmax(1) == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    return loss_sum/total, correct/max(1,total)\n",
        "\n",
        "\n",
        "best_acc = 0.0\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader, True)\n",
        "    te_loss, te_acc = run_epoch(test_loader,  False)\n",
        "    print(f\"Epoch {ep:02d} | train {tr_loss:.4f}/{tr_acc:.4f} | test {te_loss:.4f}/{te_acc:.4f}\")\n",
        "    if te_acc > best_acc:\n",
        "        best_acc = te_acc\n",
        "        torch.save({\n",
        "            \"model\": model.state_dict(),\n",
        "            \"label_map\": full.label_map,\n",
        "            \"in_dim\": D,\n",
        "            \"num_classes\": num_classes\n",
        "        }, \"/content/drive/MyDrive/code_data25/Basu/model_save/best_router_mlp_80_20.pth\")\n",
        "\n",
        "# -------- Final report (on test set) --------\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        pr = model(xb.to(DEVICE)).argmax(1).cpu().numpy()\n",
        "        y_pred.append(pr); y_true.append(yb.numpy())\n",
        "y_true = np.concatenate(y_true); y_pred = np.concatenate(y_pred)\n",
        "\n",
        "print(\"\\nTest accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"\\nPer-class report:\\n\", classification_report(y_true, y_pred, digits=4))\n",
        "print(\"\\nConfusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_true, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwmRGbrt8b5Y",
        "outputId": "70734bd1-ae2c-498f-f73d-62664b70ef84"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N=15000, D=16384, classes=30\n",
            "label_map: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29}\n",
            "\n",
            "Per-class counts (train/test):\n",
            "class 0: train=400, test=100, total=500\n",
            "class 1: train=400, test=100, total=500\n",
            "class 2: train=400, test=100, total=500\n",
            "class 3: train=400, test=100, total=500\n",
            "class 4: train=400, test=100, total=500\n",
            "class 5: train=400, test=100, total=500\n",
            "class 6: train=400, test=100, total=500\n",
            "class 7: train=400, test=100, total=500\n",
            "class 8: train=400, test=100, total=500\n",
            "class 9: train=400, test=100, total=500\n",
            "class 10: train=400, test=100, total=500\n",
            "class 11: train=400, test=100, total=500\n",
            "class 12: train=400, test=100, total=500\n",
            "class 13: train=400, test=100, total=500\n",
            "class 14: train=400, test=100, total=500\n",
            "class 15: train=400, test=100, total=500\n",
            "class 16: train=400, test=100, total=500\n",
            "class 17: train=400, test=100, total=500\n",
            "class 18: train=400, test=100, total=500\n",
            "class 19: train=400, test=100, total=500\n",
            "class 20: train=400, test=100, total=500\n",
            "class 21: train=400, test=100, total=500\n",
            "class 22: train=400, test=100, total=500\n",
            "class 23: train=400, test=100, total=500\n",
            "class 24: train=400, test=100, total=500\n",
            "class 25: train=400, test=100, total=500\n",
            "class 26: train=400, test=100, total=500\n",
            "class 27: train=400, test=100, total=500\n",
            "class 28: train=400, test=100, total=500\n",
            "class 29: train=400, test=100, total=500\n",
            "Epoch 01 | train 14.2621/0.0318 | test 9.3526/0.0353\n",
            "Epoch 02 | train 6.9568/0.0338 | test 5.2451/0.0403\n",
            "Epoch 03 | train 4.5678/0.0410 | test 4.1558/0.0503\n",
            "Epoch 04 | train 4.0816/0.0408 | test 4.0880/0.0420\n",
            "Epoch 05 | train 3.8807/0.0443 | test 4.1699/0.0450\n",
            "Epoch 06 | train 4.3032/0.0472 | test 4.2715/0.0410\n",
            "Epoch 07 | train 4.1886/0.0489 | test 4.1371/0.0683\n",
            "Epoch 08 | train 4.1117/0.0554 | test 4.1997/0.0467\n",
            "Epoch 09 | train 4.2430/0.0551 | test 4.0673/0.0413\n",
            "Epoch 10 | train 3.9113/0.0602 | test 3.8030/0.0593\n",
            "Epoch 11 | train 3.7397/0.0641 | test 3.6372/0.0640\n",
            "Epoch 12 | train 3.6806/0.0660 | test 3.7405/0.0620\n",
            "Epoch 13 | train 3.6349/0.0705 | test 3.7777/0.0667\n",
            "Epoch 14 | train 3.6811/0.0731 | test 3.7634/0.0710\n",
            "Epoch 15 | train 3.6980/0.0746 | test 3.6709/0.0607\n",
            "Epoch 16 | train 3.5784/0.0815 | test 3.6246/0.0787\n",
            "Epoch 17 | train 3.6172/0.0803 | test 3.8689/0.0760\n",
            "Epoch 18 | train 3.5306/0.0868 | test 3.4842/0.0873\n",
            "Epoch 19 | train 3.4428/0.0870 | test 3.5453/0.0633\n",
            "Epoch 20 | train 3.4234/0.0928 | test 3.4553/0.0727\n",
            "Epoch 21 | train 3.4702/0.0932 | test 3.6381/0.0777\n",
            "Epoch 22 | train 3.5004/0.0946 | test 3.4419/0.0783\n",
            "Epoch 23 | train 3.4073/0.0949 | test 3.4598/0.0910\n",
            "Epoch 24 | train 3.3632/0.0997 | test 3.2668/0.1073\n",
            "Epoch 25 | train 3.2453/0.1064 | test 3.3229/0.0833\n",
            "Epoch 26 | train 3.2713/0.1027 | test 3.3430/0.1117\n",
            "Epoch 27 | train 3.2335/0.1191 | test 3.2843/0.1120\n",
            "Epoch 28 | train 3.2537/0.1071 | test 3.2288/0.1050\n",
            "Epoch 29 | train 3.2267/0.1155 | test 3.1487/0.1177\n",
            "Epoch 30 | train 3.1593/0.1240 | test 3.1582/0.1200\n",
            "Epoch 31 | train 3.1556/0.1217 | test 3.2155/0.0870\n",
            "Epoch 32 | train 3.1452/0.1286 | test 3.2429/0.1140\n",
            "Epoch 33 | train 3.1273/0.1339 | test 3.0645/0.1367\n",
            "Epoch 34 | train 3.0331/0.1425 | test 3.0242/0.1170\n",
            "Epoch 35 | train 3.0021/0.1389 | test 3.2185/0.0860\n",
            "Epoch 36 | train 3.0408/0.1382 | test 3.0877/0.1003\n",
            "Epoch 37 | train 2.9963/0.1488 | test 3.0503/0.1377\n",
            "Epoch 38 | train 2.9643/0.1472 | test 2.9702/0.1417\n",
            "Epoch 39 | train 2.8483/0.1703 | test 2.8933/0.1517\n",
            "Epoch 40 | train 2.8121/0.1785 | test 2.8190/0.1500\n",
            "Epoch 41 | train 2.7963/0.1749 | test 2.7974/0.1760\n",
            "Epoch 42 | train 2.7694/0.1817 | test 2.9393/0.1423\n",
            "Epoch 43 | train 2.7437/0.1896 | test 2.7914/0.1607\n",
            "Epoch 44 | train 2.7287/0.1780 | test 2.8420/0.1603\n",
            "Epoch 45 | train 2.6872/0.2077 | test 2.6286/0.2290\n",
            "Epoch 46 | train 2.5787/0.2219 | test 2.6547/0.2140\n",
            "Epoch 47 | train 2.5553/0.2318 | test 2.6501/0.1977\n",
            "Epoch 48 | train 2.5932/0.2107 | test 2.6330/0.2220\n",
            "Epoch 49 | train 2.5302/0.2259 | test 2.5796/0.2290\n",
            "Epoch 50 | train 2.5688/0.2155 | test 2.5473/0.2340\n",
            "Epoch 51 | train 2.5204/0.2256 | test 2.4413/0.2553\n",
            "Epoch 52 | train 2.4437/0.2494 | test 2.5433/0.2150\n",
            "Epoch 53 | train 2.4039/0.2604 | test 2.4294/0.2433\n",
            "Epoch 54 | train 2.3528/0.2781 | test 2.5076/0.2100\n",
            "Epoch 55 | train 2.4261/0.2487 | test 2.5061/0.2223\n",
            "Epoch 56 | train 2.3217/0.2797 | test 2.3523/0.2613\n",
            "Epoch 57 | train 2.3046/0.2802 | test 2.3409/0.2587\n",
            "Epoch 58 | train 2.4010/0.2456 | test 2.4646/0.2350\n",
            "Epoch 59 | train 2.2688/0.2889 | test 2.2780/0.2700\n",
            "Epoch 60 | train 2.2078/0.3127 | test 2.2852/0.2710\n",
            "Epoch 61 | train 2.1682/0.3328 | test 2.2696/0.2883\n",
            "Epoch 62 | train 2.2843/0.2732 | test 2.1939/0.2953\n",
            "Epoch 63 | train 2.1477/0.3195 | test 2.2552/0.3127\n",
            "Epoch 64 | train 2.1147/0.3362 | test 2.1339/0.3083\n",
            "Epoch 65 | train 2.1457/0.3223 | test 2.1360/0.3257\n",
            "Epoch 66 | train 2.2107/0.3019 | test 2.3692/0.2660\n",
            "Epoch 67 | train 2.1483/0.3211 | test 2.3408/0.2587\n",
            "Epoch 68 | train 2.1247/0.3296 | test 2.5015/0.2487\n",
            "Epoch 69 | train 2.2260/0.3034 | test 2.0639/0.3790\n",
            "Epoch 70 | train 2.0193/0.3645 | test 2.0737/0.3530\n",
            "Epoch 71 | train 1.9555/0.3985 | test 1.9983/0.3783\n",
            "Epoch 72 | train 1.9094/0.4158 | test 2.1967/0.2817\n",
            "Epoch 73 | train 2.0268/0.3613 | test 2.1132/0.3233\n",
            "Epoch 74 | train 2.0249/0.3534 | test 2.0485/0.3487\n",
            "Epoch 75 | train 1.9882/0.3682 | test 2.0097/0.3447\n",
            "Epoch 76 | train 1.8331/0.4377 | test 1.9888/0.3577\n",
            "Epoch 77 | train 1.8650/0.4197 | test 2.0156/0.3443\n",
            "Epoch 78 | train 1.8791/0.4132 | test 2.0478/0.3377\n",
            "Epoch 79 | train 1.8297/0.4318 | test 1.9752/0.3563\n",
            "Epoch 80 | train 1.8966/0.3988 | test 2.6664/0.2590\n",
            "Epoch 81 | train 2.1107/0.3392 | test 2.2952/0.2890\n",
            "Epoch 82 | train 1.9752/0.3824 | test 1.9244/0.3703\n",
            "Epoch 83 | train 1.7665/0.4504 | test 1.7144/0.5060\n",
            "Epoch 84 | train 1.7313/0.4677 | test 1.7367/0.4820\n",
            "Epoch 85 | train 1.8266/0.4190 | test 2.0666/0.3353\n",
            "Epoch 86 | train 1.8049/0.4204 | test 1.6745/0.5450\n",
            "Epoch 87 | train 1.6361/0.5192 | test 1.8942/0.4013\n",
            "Epoch 88 | train 1.6848/0.4723 | test 1.7747/0.4527\n",
            "Epoch 89 | train 1.6361/0.5213 | test 1.7295/0.4793\n",
            "Epoch 90 | train 1.6440/0.4856 | test 1.5643/0.5723\n",
            "Epoch 91 | train 1.6318/0.4995 | test 1.6228/0.5463\n",
            "Epoch 92 | train 1.7019/0.4597 | test 1.7823/0.4733\n",
            "Epoch 93 | train 1.7379/0.4361 | test 1.7026/0.4540\n",
            "Epoch 94 | train 1.6775/0.4719 | test 1.6175/0.5147\n",
            "Epoch 95 | train 1.7465/0.4437 | test 2.0574/0.3040\n",
            "Epoch 96 | train 1.8405/0.4151 | test 1.9192/0.3857\n",
            "Epoch 97 | train 1.6601/0.4748 | test 1.7553/0.4633\n",
            "Epoch 98 | train 1.5810/0.5118 | test 1.7753/0.4380\n",
            "Epoch 99 | train 1.6687/0.4733 | test 1.7185/0.4450\n",
            "Epoch 100 | train 1.5621/0.5100 | test 1.8639/0.3827\n",
            "Epoch 101 | train 1.6908/0.4622 | test 1.5639/0.5087\n",
            "Epoch 102 | train 1.5581/0.5108 | test 1.5201/0.5690\n",
            "Epoch 103 | train 1.4276/0.5866 | test 1.4733/0.5647\n",
            "Epoch 104 | train 1.4247/0.5716 | test 1.6305/0.4757\n",
            "Epoch 105 | train 1.5472/0.5216 | test 1.6542/0.4610\n",
            "Epoch 106 | train 1.4325/0.5570 | test 1.3873/0.6320\n",
            "Epoch 107 | train 1.4709/0.5458 | test 1.5303/0.5280\n",
            "Epoch 108 | train 1.4612/0.5403 | test 1.4398/0.6000\n",
            "Epoch 109 | train 1.3293/0.6152 | test 1.4744/0.5513\n",
            "Epoch 110 | train 1.4001/0.5636 | test 1.3994/0.6013\n",
            "Epoch 111 | train 1.4461/0.5443 | test 1.4383/0.5830\n",
            "Epoch 112 | train 1.5416/0.4994 | test 1.6270/0.4717\n",
            "Epoch 113 | train 1.6976/0.4419 | test 1.5540/0.5193\n",
            "Epoch 114 | train 1.4435/0.5396 | test 1.5967/0.4987\n",
            "Epoch 115 | train 1.6959/0.4430 | test 1.6411/0.5030\n",
            "Epoch 116 | train 1.3449/0.5937 | test 1.3931/0.5733\n",
            "Epoch 117 | train 1.3715/0.5734 | test 1.4890/0.5167\n",
            "Epoch 118 | train 1.4187/0.5468 | test 1.4525/0.5490\n",
            "Epoch 119 | train 1.2412/0.6464 | test 1.2594/0.6437\n",
            "Epoch 120 | train 1.3912/0.5633 | test 1.5550/0.5127\n",
            "Epoch 121 | train 1.2485/0.6302 | test 1.2492/0.6513\n",
            "Epoch 122 | train 1.1637/0.6758 | test 1.3127/0.6097\n",
            "Epoch 123 | train 1.3061/0.6021 | test 2.1074/0.3657\n",
            "Epoch 124 | train 1.5596/0.5011 | test 1.3549/0.5987\n",
            "Epoch 125 | train 1.2252/0.6277 | test 1.2380/0.6547\n",
            "Epoch 126 | train 1.1747/0.6579 | test 1.2953/0.6050\n",
            "Epoch 127 | train 1.2028/0.6306 | test 1.3894/0.5677\n",
            "Epoch 128 | train 1.3624/0.5602 | test 1.3300/0.5850\n",
            "Epoch 129 | train 1.1496/0.6683 | test 1.2890/0.6563\n",
            "Epoch 130 | train 1.4325/0.5379 | test 1.6287/0.4813\n",
            "Epoch 131 | train 1.3690/0.5597 | test 1.6020/0.5157\n",
            "Epoch 132 | train 1.3047/0.5780 | test 1.2336/0.6323\n",
            "Epoch 133 | train 1.0884/0.6925 | test 1.1391/0.6813\n",
            "Epoch 134 | train 1.0610/0.7049 | test 1.1942/0.6673\n",
            "Epoch 135 | train 1.1366/0.6519 | test 1.1616/0.6857\n",
            "Epoch 136 | train 1.1710/0.6272 | test 1.3170/0.5947\n",
            "Epoch 137 | train 1.1330/0.6553 | test 1.3305/0.5893\n",
            "Epoch 138 | train 1.2904/0.5956 | test 1.9357/0.4207\n",
            "Epoch 139 | train 1.3184/0.5767 | test 1.2627/0.6400\n",
            "Epoch 140 | train 1.1220/0.6604 | test 1.6186/0.4947\n",
            "Epoch 141 | train 1.2702/0.6026 | test 1.2590/0.5843\n",
            "Epoch 142 | train 1.4581/0.5144 | test 1.3520/0.5387\n",
            "Epoch 143 | train 1.2010/0.6033 | test 1.3952/0.5260\n",
            "Epoch 144 | train 1.1515/0.6286 | test 1.3047/0.5897\n",
            "Epoch 145 | train 1.0132/0.7117 | test 0.9897/0.7620\n",
            "Epoch 146 | train 1.0048/0.7063 | test 1.3268/0.5883\n",
            "Epoch 147 | train 1.1347/0.6416 | test 1.1538/0.6463\n",
            "Epoch 148 | train 1.1517/0.6285 | test 1.2027/0.6137\n",
            "Epoch 149 | train 0.9824/0.7131 | test 1.0188/0.7130\n",
            "Epoch 150 | train 0.9753/0.7232 | test 1.2761/0.5957\n",
            "Epoch 151 | train 1.3067/0.5672 | test 1.2749/0.5817\n",
            "Epoch 152 | train 1.2237/0.5964 | test 1.2050/0.6220\n",
            "Epoch 153 | train 1.0049/0.6991 | test 1.1074/0.6683\n",
            "Epoch 154 | train 0.9274/0.7392 | test 0.9916/0.7397\n",
            "Epoch 155 | train 0.9298/0.7322 | test 0.9673/0.7430\n",
            "Epoch 156 | train 0.9237/0.7371 | test 1.6934/0.4313\n",
            "Epoch 157 | train 1.7373/0.4848 | test 2.0207/0.4400\n",
            "Epoch 158 | train 1.3572/0.5505 | test 1.2253/0.6163\n",
            "Epoch 159 | train 0.9695/0.7132 | test 1.0594/0.7007\n",
            "Epoch 160 | train 0.8604/0.7690 | test 0.9865/0.7493\n",
            "Epoch 161 | train 0.9949/0.6951 | test 1.0978/0.6810\n",
            "Epoch 162 | train 0.8912/0.7517 | test 0.8546/0.8203\n",
            "Epoch 163 | train 0.9094/0.7360 | test 1.3083/0.5933\n",
            "Epoch 164 | train 0.9611/0.7100 | test 1.1020/0.6583\n",
            "Epoch 165 | train 0.8996/0.7399 | test 1.1269/0.6520\n",
            "Epoch 166 | train 0.9696/0.6981 | test 1.5347/0.4803\n",
            "Epoch 167 | train 1.3198/0.5734 | test 1.2635/0.5837\n",
            "Epoch 168 | train 1.1092/0.6341 | test 1.2530/0.6370\n",
            "Epoch 169 | train 1.0214/0.6701 | test 1.1483/0.6373\n",
            "Epoch 170 | train 1.3340/0.5693 | test 1.5848/0.5017\n",
            "Epoch 171 | train 1.0431/0.6676 | test 1.0945/0.6527\n",
            "Epoch 172 | train 0.8478/0.7602 | test 0.8677/0.7840\n",
            "Epoch 173 | train 0.7865/0.7914 | test 0.8225/0.8173\n",
            "Epoch 174 | train 0.7237/0.8217 | test 0.9192/0.7550\n",
            "Epoch 175 | train 0.7417/0.8072 | test 0.9920/0.7233\n",
            "Epoch 176 | train 0.9192/0.7093 | test 0.9268/0.7437\n",
            "Epoch 177 | train 0.8588/0.7317 | test 0.9296/0.7247\n",
            "Epoch 178 | train 0.8938/0.7166 | test 1.0103/0.6793\n",
            "Epoch 179 | train 0.7699/0.7792 | test 0.8489/0.7870\n",
            "Epoch 180 | train 0.8802/0.7262 | test 0.9193/0.7380\n",
            "Epoch 181 | train 0.7631/0.7804 | test 0.9660/0.7140\n",
            "Epoch 182 | train 0.8864/0.7207 | test 1.3975/0.5460\n",
            "Epoch 183 | train 1.9529/0.4461 | test 1.5455/0.4763\n",
            "Epoch 184 | train 1.4900/0.5217 | test 1.2788/0.5850\n",
            "Epoch 185 | train 1.1930/0.6057 | test 1.0768/0.6857\n",
            "Epoch 186 | train 0.8066/0.7642 | test 0.8351/0.7970\n",
            "Epoch 187 | train 0.7022/0.8207 | test 0.8852/0.7490\n",
            "Epoch 188 | train 0.6575/0.8391 | test 0.7271/0.8363\n",
            "Epoch 189 | train 0.6225/0.8578 | test 0.7558/0.8137\n",
            "Epoch 190 | train 0.8355/0.7471 | test 1.0592/0.6813\n",
            "Epoch 191 | train 1.0419/0.6583 | test 1.3657/0.5647\n",
            "Epoch 192 | train 1.0372/0.6669 | test 0.9211/0.7257\n",
            "Epoch 193 | train 0.7676/0.7762 | test 0.9022/0.7400\n",
            "Epoch 194 | train 0.7566/0.7786 | test 0.7957/0.7987\n",
            "Epoch 195 | train 0.9026/0.7057 | test 1.7655/0.4873\n",
            "Epoch 196 | train 2.1567/0.4465 | test 1.9692/0.4547\n",
            "Epoch 197 | train 1.4217/0.5579 | test 1.1205/0.6503\n",
            "Epoch 198 | train 0.7840/0.7705 | test 0.7436/0.8303\n",
            "Epoch 199 | train 0.6424/0.8350 | test 0.7586/0.8067\n",
            "Epoch 200 | train 0.6503/0.8303 | test 0.6907/0.8337\n",
            "Epoch 201 | train 0.6004/0.8553 | test 0.7157/0.8347\n",
            "Epoch 202 | train 0.6622/0.8288 | test 1.1356/0.6520\n",
            "Epoch 203 | train 0.7978/0.7522 | test 0.9166/0.7257\n",
            "Epoch 204 | train 0.6866/0.8055 | test 0.7341/0.8127\n",
            "Epoch 205 | train 0.6384/0.8327 | test 0.7576/0.8040\n",
            "Epoch 206 | train 0.6201/0.8358 | test 0.6851/0.8497\n",
            "Epoch 207 | train 0.6881/0.8076 | test 1.0040/0.6970\n",
            "Epoch 208 | train 0.9762/0.6709 | test 1.0096/0.6810\n",
            "Epoch 209 | train 0.7221/0.7834 | test 0.7775/0.7813\n",
            "Epoch 210 | train 0.5698/0.8589 | test 0.7118/0.7973\n",
            "Epoch 211 | train 0.7691/0.7752 | test 1.1711/0.6390\n",
            "Epoch 212 | train 0.8601/0.7273 | test 0.9136/0.7197\n",
            "Epoch 213 | train 0.7872/0.7529 | test 0.9869/0.6787\n",
            "Epoch 214 | train 1.0360/0.6567 | test 0.8587/0.7630\n",
            "Epoch 215 | train 0.9305/0.6933 | test 0.8453/0.7490\n",
            "Epoch 216 | train 0.7887/0.7418 | test 1.1012/0.6320\n",
            "Epoch 217 | train 0.7663/0.7513 | test 0.7261/0.8013\n",
            "Epoch 218 | train 0.5943/0.8431 | test 0.6855/0.8327\n",
            "Epoch 219 | train 0.5321/0.8703 | test 0.6575/0.8360\n",
            "Epoch 220 | train 0.5258/0.8665 | test 0.6348/0.8683\n",
            "Epoch 221 | train 0.5687/0.8469 | test 0.7707/0.7837\n",
            "Epoch 222 | train 2.4114/0.5003 | test 4.8609/0.2650\n",
            "Epoch 223 | train 3.2460/0.3439 | test 2.9378/0.4043\n",
            "Epoch 224 | train 2.0571/0.4822 | test 1.1595/0.6383\n",
            "Epoch 225 | train 0.8449/0.7330 | test 0.8381/0.7443\n",
            "Epoch 226 | train 0.6349/0.8135 | test 0.8439/0.7273\n",
            "Epoch 227 | train 0.6360/0.8110 | test 0.7223/0.8043\n",
            "Epoch 228 | train 0.5293/0.8679 | test 0.6391/0.8507\n",
            "Epoch 229 | train 0.5735/0.8409 | test 0.7715/0.7807\n",
            "Epoch 230 | train 0.5877/0.8355 | test 0.6819/0.8183\n",
            "Epoch 231 | train 0.5263/0.8635 | test 0.6069/0.8587\n",
            "Epoch 232 | train 0.5015/0.8780 | test 0.5697/0.8817\n",
            "Epoch 233 | train 0.5315/0.8584 | test 0.7091/0.8137\n",
            "Epoch 234 | train 0.4918/0.8817 | test 0.5681/0.8797\n",
            "Epoch 235 | train 0.5299/0.8583 | test 3.3191/0.4177\n",
            "Epoch 236 | train 2.3877/0.4331 | test 2.0812/0.4067\n",
            "Epoch 237 | train 1.5972/0.5326 | test 0.9511/0.7220\n",
            "Epoch 238 | train 0.6854/0.7936 | test 0.6334/0.8477\n",
            "Epoch 239 | train 0.4857/0.8838 | test 0.5601/0.8930\n",
            "Epoch 240 | train 0.4589/0.8966 | test 0.5804/0.8717\n",
            "Epoch 241 | train 0.4598/0.8927 | test 0.6175/0.8477\n",
            "Epoch 242 | train 0.4653/0.8898 | test 0.5082/0.9017\n",
            "Epoch 243 | train 0.4380/0.9013 | test 0.5928/0.8590\n",
            "Epoch 244 | train 0.4724/0.8812 | test 0.6335/0.8370\n",
            "Epoch 245 | train 0.4887/0.8749 | test 0.6160/0.8467\n",
            "Epoch 246 | train 0.4766/0.8802 | test 0.5462/0.8787\n",
            "Epoch 247 | train 0.7108/0.8265 | test 2.2807/0.4437\n",
            "Epoch 248 | train 3.3492/0.3689 | test 3.5796/0.3220\n",
            "Epoch 249 | train 1.5981/0.5443 | test 1.1553/0.6523\n",
            "Epoch 250 | train 0.7517/0.7628 | test 0.7542/0.7880\n",
            "Epoch 251 | train 0.5550/0.8473 | test 0.5561/0.8810\n",
            "Epoch 252 | train 0.4805/0.8854 | test 0.7550/0.7790\n",
            "Epoch 253 | train 0.5300/0.8538 | test 0.5853/0.8753\n",
            "Epoch 254 | train 0.4713/0.8869 | test 0.5668/0.8777\n",
            "Epoch 255 | train 0.4441/0.8966 | test 0.5253/0.8870\n",
            "Epoch 256 | train 0.4477/0.8930 | test 0.6045/0.8530\n",
            "Epoch 257 | train 0.4666/0.8846 | test 0.5427/0.8747\n",
            "Epoch 258 | train 0.4260/0.9021 | test 0.5167/0.8987\n",
            "Epoch 259 | train 0.3969/0.9121 | test 0.5095/0.8923\n",
            "Epoch 260 | train 0.4176/0.9001 | test 0.5565/0.8740\n",
            "Epoch 261 | train 0.4573/0.8812 | test 0.5363/0.8723\n",
            "Epoch 262 | train 0.4604/0.8790 | test 0.5770/0.8570\n",
            "Epoch 263 | train 0.4535/0.8770 | test 0.5528/0.8687\n",
            "Epoch 264 | train 0.4559/0.8810 | test 0.5593/0.8640\n",
            "Epoch 265 | train 0.4493/0.8861 | test 0.5156/0.8860\n",
            "Epoch 266 | train 0.4276/0.8913 | test 0.4908/0.8957\n",
            "Epoch 267 | train 1.6133/0.7282 | test 6.0281/0.2040\n",
            "Epoch 268 | train 5.1235/0.2817 | test 6.6652/0.2183\n",
            "Epoch 269 | train 6.6591/0.2722 | test 5.1973/0.3190\n",
            "Epoch 270 | train 4.2978/0.3593 | test 3.9341/0.3860\n",
            "Epoch 271 | train 2.5595/0.4672 | test 1.7217/0.5560\n",
            "Epoch 272 | train 1.0367/0.6835 | test 0.9448/0.7193\n",
            "Epoch 273 | train 0.6204/0.8092 | test 0.5962/0.8610\n",
            "Epoch 274 | train 0.4768/0.8714 | test 0.5482/0.8850\n",
            "Epoch 275 | train 0.4231/0.9052 | test 0.5112/0.8907\n",
            "Epoch 276 | train 0.4101/0.9020 | test 0.5065/0.8837\n",
            "Epoch 277 | train 0.3931/0.9128 | test 0.5248/0.8790\n",
            "Epoch 278 | train 0.3905/0.9087 | test 0.4721/0.9047\n",
            "Epoch 279 | train 0.3890/0.9094 | test 0.5090/0.8927\n",
            "Epoch 280 | train 0.4101/0.8999 | test 0.5121/0.9010\n",
            "Epoch 281 | train 0.3942/0.9076 | test 0.4640/0.9037\n",
            "Epoch 282 | train 0.4154/0.8964 | test 0.6893/0.8007\n",
            "Epoch 283 | train 0.6295/0.8003 | test 0.7132/0.7890\n",
            "Epoch 284 | train 0.4555/0.8754 | test 0.5285/0.8840\n",
            "Epoch 285 | train 0.4354/0.8861 | test 0.5640/0.8633\n",
            "Epoch 286 | train 0.4885/0.8577 | test 0.7464/0.7703\n",
            "Epoch 287 | train 0.4354/0.8838 | test 0.5388/0.8790\n",
            "Epoch 288 | train 0.3752/0.9123 | test 0.5057/0.8880\n",
            "Epoch 289 | train 0.3916/0.9044 | test 0.4605/0.9050\n",
            "Epoch 290 | train 0.4065/0.8992 | test 0.5589/0.8637\n",
            "Epoch 291 | train 0.6170/0.8017 | test 0.6917/0.8053\n",
            "Epoch 292 | train 0.5186/0.8459 | test 0.5552/0.8640\n",
            "Epoch 293 | train 0.4092/0.8961 | test 0.5566/0.8567\n",
            "Epoch 294 | train 0.4208/0.8878 | test 0.5725/0.8617\n",
            "Epoch 295 | train 0.4522/0.8736 | test 0.5976/0.8433\n",
            "Epoch 296 | train 0.4418/0.8774 | test 0.5616/0.8587\n",
            "Epoch 297 | train 0.4640/0.8622 | test 0.5999/0.8387\n",
            "Epoch 298 | train 0.4593/0.8684 | test 0.6112/0.8303\n",
            "Epoch 299 | train 0.4307/0.8776 | test 0.5052/0.8803\n",
            "Epoch 300 | train 0.4384/0.8729 | test 0.5981/0.8357\n",
            "Epoch 301 | train 4.0381/0.4357 | test 6.7375/0.2670\n",
            "Epoch 302 | train 4.4684/0.3227 | test 2.4829/0.4970\n",
            "Epoch 303 | train 1.5909/0.5620 | test 1.0705/0.6563\n",
            "Epoch 304 | train 0.7999/0.7368 | test 0.7405/0.7950\n",
            "Epoch 305 | train 0.4852/0.8615 | test 0.5236/0.8790\n",
            "Epoch 306 | train 0.3908/0.9034 | test 0.5533/0.8717\n",
            "Epoch 307 | train 0.3818/0.9032 | test 0.4994/0.8860\n",
            "Epoch 308 | train 0.3752/0.9103 | test 0.5438/0.8753\n",
            "Epoch 309 | train 0.3696/0.9104 | test 0.4743/0.9043\n",
            "Epoch 310 | train 0.3780/0.9052 | test 0.4470/0.9153\n",
            "Epoch 311 | train 0.3554/0.9174 | test 0.5118/0.8790\n",
            "Epoch 312 | train 0.3956/0.8948 | test 0.4986/0.8817\n",
            "Epoch 313 | train 0.3387/0.9235 | test 0.4248/0.9120\n",
            "Epoch 314 | train 0.3475/0.9149 | test 0.4534/0.9147\n",
            "Epoch 315 | train 0.3504/0.9145 | test 0.4721/0.8997\n",
            "Epoch 316 | train 0.3537/0.9120 | test 0.4449/0.9150\n",
            "Epoch 317 | train 0.3503/0.9147 | test 0.4772/0.8893\n",
            "Epoch 318 | train 0.3906/0.8945 | test 0.5068/0.8787\n",
            "Epoch 319 | train 0.3628/0.9058 | test 0.4641/0.9003\n",
            "Epoch 320 | train 0.3471/0.9119 | test 0.4890/0.8833\n",
            "Epoch 321 | train 0.3493/0.9127 | test 0.4546/0.9030\n",
            "Epoch 322 | train 0.3342/0.9195 | test 0.4186/0.9167\n",
            "Epoch 323 | train 0.3271/0.9218 | test 0.4567/0.9057\n",
            "Epoch 324 | train 0.3770/0.9000 | test 0.5497/0.8490\n",
            "Epoch 325 | train 1.1942/0.7517 | test 4.7050/0.4073\n",
            "Epoch 326 | train 2.6350/0.4635 | test 1.2770/0.6557\n",
            "Epoch 327 | train 0.9436/0.7074 | test 0.6326/0.8293\n",
            "Epoch 328 | train 0.4232/0.8812 | test 0.5106/0.8793\n",
            "Epoch 329 | train 0.3833/0.9012 | test 0.4712/0.8817\n",
            "Epoch 330 | train 0.3348/0.9193 | test 0.4234/0.9190\n",
            "Epoch 331 | train 0.3222/0.9291 | test 0.3958/0.9350\n",
            "Epoch 332 | train 0.3205/0.9234 | test 0.4366/0.9023\n",
            "Epoch 333 | train 0.3335/0.9197 | test 0.4959/0.8867\n",
            "Epoch 334 | train 0.3521/0.9078 | test 0.4835/0.8873\n",
            "Epoch 335 | train 0.3856/0.8918 | test 0.5081/0.8663\n",
            "Epoch 336 | train 0.3641/0.9049 | test 0.4210/0.9100\n",
            "Epoch 337 | train 0.3375/0.9148 | test 0.4321/0.9113\n",
            "Epoch 338 | train 0.3236/0.9193 | test 0.3913/0.9160\n",
            "Epoch 339 | train 0.3331/0.9132 | test 0.5183/0.8623\n",
            "Epoch 340 | train 0.3734/0.9002 | test 0.4351/0.8987\n",
            "Epoch 341 | train 0.4187/0.8748 | test 0.5787/0.8323\n",
            "Epoch 342 | train 3.8058/0.4272 | test 4.6522/0.2870\n",
            "Epoch 343 | train 3.9922/0.3628 | test 2.2797/0.4630\n",
            "Epoch 344 | train 1.6531/0.5603 | test 1.2290/0.6517\n",
            "Epoch 345 | train 0.9938/0.7048 | test 0.7993/0.7947\n",
            "Epoch 346 | train 0.5064/0.8476 | test 0.4668/0.9003\n",
            "Epoch 347 | train 0.3535/0.9127 | test 0.4345/0.9190\n",
            "Epoch 348 | train 0.3341/0.9204 | test 0.4602/0.9043\n",
            "Epoch 349 | train 0.3214/0.9227 | test 0.4170/0.9193\n",
            "Epoch 350 | train 0.3118/0.9276 | test 0.4176/0.9180\n",
            "Epoch 351 | train 0.3248/0.9233 | test 0.4313/0.9163\n",
            "Epoch 352 | train 0.3652/0.9002 | test 0.4141/0.9197\n",
            "Epoch 353 | train 0.3031/0.9332 | test 0.3896/0.9220\n",
            "Epoch 354 | train 0.3003/0.9313 | test 0.4249/0.9030\n",
            "Epoch 355 | train 0.3026/0.9267 | test 0.4195/0.9087\n",
            "Epoch 356 | train 0.2975/0.9290 | test 0.4067/0.9153\n",
            "Epoch 357 | train 0.3064/0.9268 | test 0.4084/0.9137\n",
            "Epoch 358 | train 0.2914/0.9340 | test 0.3994/0.9143\n",
            "Epoch 359 | train 0.2997/0.9301 | test 0.4257/0.9043\n",
            "Epoch 360 | train 0.2995/0.9287 | test 0.3994/0.9147\n",
            "Epoch 361 | train 0.2854/0.9360 | test 0.3918/0.9137\n",
            "Epoch 362 | train 0.3015/0.9256 | test 0.4133/0.9157\n",
            "Epoch 363 | train 0.3205/0.9204 | test 0.4050/0.9047\n",
            "Epoch 364 | train 0.2906/0.9323 | test 0.4140/0.9147\n",
            "Epoch 365 | train 0.3007/0.9253 | test 0.4373/0.9037\n",
            "Epoch 366 | train 0.3670/0.8972 | test 0.4883/0.8790\n",
            "Epoch 367 | train 0.4677/0.8519 | test 0.6279/0.8320\n",
            "Epoch 368 | train 0.4243/0.8666 | test 0.6966/0.7857\n",
            "Epoch 369 | train 3.4677/0.4238 | test 3.1038/0.4023\n",
            "Epoch 370 | train 2.1703/0.5213 | test 1.0944/0.6863\n",
            "Epoch 371 | train 0.9198/0.7028 | test 1.0322/0.6883\n",
            "Epoch 372 | train 0.5509/0.8222 | test 0.5598/0.8587\n",
            "Epoch 373 | train 0.4090/0.8787 | test 0.4567/0.8843\n",
            "Epoch 374 | train 0.3309/0.9113 | test 0.4045/0.9157\n",
            "Epoch 375 | train 0.2991/0.9275 | test 0.4111/0.9170\n",
            "Epoch 376 | train 0.2984/0.9296 | test 0.4076/0.9130\n",
            "Epoch 377 | train 0.2847/0.9307 | test 0.3914/0.9233\n",
            "Epoch 378 | train 0.2971/0.9277 | test 0.4075/0.9087\n",
            "Epoch 379 | train 0.3224/0.9153 | test 0.4371/0.8907\n",
            "Epoch 380 | train 0.3935/0.8796 | test 0.5285/0.8553\n",
            "Epoch 381 | train 0.7278/0.7651 | test 1.1313/0.6403\n",
            "Epoch 382 | train 0.5916/0.7974 | test 0.5195/0.8710\n",
            "Epoch 383 | train 0.3390/0.9046 | test 0.3961/0.9143\n",
            "Epoch 384 | train 0.2855/0.9264 | test 0.3698/0.9290\n",
            "Epoch 385 | train 0.2729/0.9355 | test 0.3920/0.9163\n",
            "Epoch 386 | train 0.2632/0.9380 | test 0.3646/0.9273\n",
            "Epoch 387 | train 0.2666/0.9343 | test 0.4418/0.8940\n",
            "Epoch 388 | train 0.2802/0.9300 | test 0.3622/0.9267\n",
            "Epoch 389 | train 0.2745/0.9344 | test 0.3997/0.9047\n",
            "Epoch 390 | train 0.2824/0.9265 | test 0.4000/0.9047\n",
            "Epoch 391 | train 0.2734/0.9313 | test 0.3882/0.9160\n",
            "Epoch 392 | train 0.2864/0.9292 | test 0.4832/0.8790\n",
            "Epoch 393 | train 3.2048/0.5292 | test 5.2866/0.2977\n",
            "Epoch 394 | train 6.5803/0.2707 | test 6.1871/0.2813\n",
            "Epoch 395 | train 2.9900/0.4867 | test 1.3070/0.6667\n",
            "Epoch 396 | train 0.7795/0.7608 | test 0.7032/0.7987\n",
            "Epoch 397 | train 0.5220/0.8332 | test 0.5477/0.8553\n",
            "Epoch 398 | train 0.3624/0.9025 | test 0.5856/0.8437\n",
            "Epoch 399 | train 0.3637/0.8988 | test 0.4720/0.8780\n",
            "Epoch 400 | train 0.3031/0.9212 | test 0.4032/0.9163\n",
            "Epoch 401 | train 0.3172/0.9202 | test 0.4522/0.9027\n",
            "Epoch 402 | train 0.2935/0.9263 | test 0.4018/0.9117\n",
            "Epoch 403 | train 0.2768/0.9346 | test 0.3761/0.9347\n",
            "Epoch 404 | train 0.2827/0.9305 | test 0.3564/0.9303\n",
            "Epoch 405 | train 0.2694/0.9343 | test 0.3734/0.9220\n",
            "Epoch 406 | train 0.2925/0.9230 | test 0.4077/0.9080\n",
            "Epoch 407 | train 0.2931/0.9260 | test 0.3864/0.9193\n",
            "Epoch 408 | train 0.2671/0.9370 | test 0.3815/0.9073\n",
            "Epoch 409 | train 0.2563/0.9408 | test 0.3672/0.9290\n",
            "Epoch 410 | train 0.2793/0.9272 | test 0.3570/0.9310\n",
            "Epoch 411 | train 0.2691/0.9353 | test 0.3921/0.9203\n",
            "Epoch 412 | train 0.2990/0.9200 | test 0.4431/0.8920\n",
            "Epoch 413 | train 0.3093/0.9118 | test 0.4283/0.9003\n",
            "Epoch 414 | train 0.2893/0.9231 | test 0.4456/0.8933\n",
            "Epoch 415 | train 0.3388/0.9023 | test 0.5129/0.8707\n",
            "Epoch 416 | train 0.3114/0.9143 | test 0.3959/0.9157\n",
            "Epoch 417 | train 0.2642/0.9350 | test 0.4324/0.8977\n",
            "Epoch 418 | train 0.2990/0.9189 | test 0.3657/0.9240\n",
            "Epoch 419 | train 0.2912/0.9217 | test 0.3690/0.9230\n",
            "Epoch 420 | train 0.2462/0.9433 | test 0.4072/0.9070\n",
            "Epoch 421 | train 0.2626/0.9337 | test 0.3994/0.9033\n",
            "Epoch 422 | train 0.2855/0.9224 | test 0.4058/0.9120\n",
            "Epoch 423 | train 0.3951/0.8798 | test 0.7727/0.7680\n",
            "Epoch 424 | train 6.4282/0.3294 | test 6.5645/0.2777\n",
            "Epoch 425 | train 3.2839/0.4365 | test 1.3074/0.6263\n",
            "Epoch 426 | train 0.8481/0.7330 | test 0.7384/0.7683\n",
            "Epoch 427 | train 0.4409/0.8638 | test 0.4376/0.9020\n",
            "Epoch 428 | train 0.3108/0.9147 | test 0.4369/0.9000\n",
            "Epoch 429 | train 0.3020/0.9175 | test 0.4176/0.9017\n",
            "Epoch 430 | train 0.2841/0.9288 | test 0.3756/0.9180\n",
            "Epoch 431 | train 0.2899/0.9236 | test 0.4263/0.9047\n",
            "Epoch 432 | train 0.3025/0.9203 | test 0.3950/0.9140\n",
            "Epoch 433 | train 0.2791/0.9277 | test 0.3830/0.9260\n",
            "Epoch 434 | train 0.2995/0.9157 | test 0.3754/0.9233\n",
            "Epoch 435 | train 0.2720/0.9320 | test 0.3519/0.9310\n",
            "Epoch 436 | train 0.2482/0.9407 | test 0.3442/0.9340\n",
            "Epoch 437 | train 0.2371/0.9414 | test 0.3569/0.9187\n",
            "Epoch 438 | train 0.2556/0.9376 | test 0.3460/0.9280\n",
            "Epoch 439 | train 0.2474/0.9397 | test 0.3609/0.9220\n",
            "Epoch 440 | train 0.2447/0.9421 | test 0.3583/0.9220\n",
            "Epoch 441 | train 0.2433/0.9410 | test 0.3666/0.9243\n",
            "Epoch 442 | train 0.2446/0.9397 | test 0.3483/0.9243\n",
            "Epoch 443 | train 0.2346/0.9449 | test 0.3688/0.9187\n",
            "Epoch 444 | train 0.2436/0.9397 | test 0.3664/0.9193\n",
            "Epoch 445 | train 0.2360/0.9390 | test 0.3373/0.9340\n",
            "Epoch 446 | train 0.2805/0.9219 | test 0.7008/0.7760\n",
            "Epoch 447 | train 0.3939/0.8742 | test 0.5065/0.8660\n",
            "Epoch 448 | train 0.3158/0.9067 | test 0.4179/0.8960\n",
            "Epoch 449 | train 0.2782/0.9245 | test 0.3524/0.9310\n",
            "Epoch 450 | train 0.2866/0.9220 | test 0.4569/0.8833\n",
            "Epoch 451 | train 0.3389/0.8952 | test 0.7684/0.7653\n",
            "Epoch 452 | train 5.5894/0.4164 | test 7.8363/0.2463\n",
            "Epoch 453 | train 4.8090/0.3581 | test 2.5430/0.5077\n",
            "Epoch 454 | train 1.6419/0.5845 | test 1.1459/0.7157\n",
            "Epoch 455 | train 0.6921/0.7809 | test 0.5810/0.8433\n",
            "Epoch 456 | train 0.3933/0.8855 | test 0.4262/0.9057\n",
            "Epoch 457 | train 0.2950/0.9211 | test 0.3764/0.9210\n",
            "Epoch 458 | train 0.2785/0.9235 | test 0.3981/0.9087\n",
            "Epoch 459 | train 0.2591/0.9351 | test 0.3737/0.9160\n",
            "Epoch 460 | train 0.2574/0.9354 | test 0.3737/0.9210\n",
            "Epoch 461 | train 0.2543/0.9333 | test 0.3449/0.9303\n",
            "Epoch 462 | train 0.2270/0.9483 | test 0.3327/0.9373\n",
            "Epoch 463 | train 0.2326/0.9437 | test 0.3473/0.9353\n",
            "Epoch 464 | train 0.2436/0.9399 | test 0.3686/0.9183\n",
            "Epoch 465 | train 0.2347/0.9417 | test 0.3342/0.9290\n",
            "Epoch 466 | train 0.2225/0.9482 | test 0.3206/0.9400\n",
            "Epoch 467 | train 0.2311/0.9449 | test 0.3265/0.9410\n",
            "Epoch 468 | train 0.2216/0.9469 | test 0.3390/0.9327\n",
            "Epoch 469 | train 0.2211/0.9473 | test 0.3475/0.9270\n",
            "Epoch 470 | train 0.2279/0.9438 | test 0.3640/0.9137\n",
            "Epoch 471 | train 0.2331/0.9401 | test 0.3266/0.9407\n",
            "Epoch 472 | train 0.2315/0.9416 | test 0.3604/0.9117\n",
            "Epoch 473 | train 0.2414/0.9347 | test 0.3480/0.9240\n",
            "Epoch 474 | train 0.2127/0.9505 | test 0.3223/0.9380\n",
            "Epoch 475 | train 0.2148/0.9492 | test 0.3182/0.9387\n",
            "Epoch 476 | train 0.2202/0.9453 | test 0.3751/0.9177\n",
            "Epoch 477 | train 0.2329/0.9396 | test 0.3333/0.9343\n",
            "Epoch 478 | train 0.2622/0.9270 | test 0.4286/0.8877\n",
            "Epoch 479 | train 0.3442/0.8938 | test 0.4100/0.8970\n",
            "Epoch 480 | train 0.2834/0.9187 | test 0.4523/0.8897\n",
            "Epoch 481 | train 0.2870/0.9169 | test 0.4013/0.8997\n",
            "Epoch 482 | train 0.2631/0.9280 | test 0.3066/0.9447\n",
            "Epoch 483 | train 0.2323/0.9414 | test 0.3139/0.9410\n",
            "Epoch 484 | train 0.2321/0.9383 | test 0.7074/0.7913\n",
            "Epoch 485 | train 7.7607/0.3273 | test 13.1297/0.1703\n",
            "Epoch 486 | train 8.3127/0.2524 | test 5.4394/0.2993\n",
            "Epoch 487 | train 2.6114/0.5178 | test 1.2373/0.6513\n",
            "Epoch 488 | train 0.7975/0.7546 | test 0.5799/0.8567\n",
            "Epoch 489 | train 0.3909/0.8832 | test 0.4487/0.8903\n",
            "Epoch 490 | train 0.3083/0.9145 | test 0.4004/0.9143\n",
            "Epoch 491 | train 0.3238/0.9088 | test 0.4464/0.8960\n",
            "Epoch 492 | train 0.2693/0.9286 | test 0.3444/0.9360\n",
            "Epoch 493 | train 0.2608/0.9339 | test 0.3709/0.9200\n",
            "Epoch 494 | train 0.2700/0.9276 | test 0.3489/0.9283\n",
            "Epoch 495 | train 0.2618/0.9317 | test 0.4017/0.9100\n",
            "Epoch 496 | train 0.2590/0.9307 | test 0.3574/0.9260\n",
            "Epoch 497 | train 0.2478/0.9366 | test 0.3489/0.9363\n",
            "Epoch 498 | train 0.2401/0.9369 | test 0.3299/0.9400\n",
            "Epoch 499 | train 0.2375/0.9347 | test 0.3495/0.9190\n",
            "Epoch 500 | train 0.2253/0.9417 | test 0.3130/0.9390\n",
            "Epoch 501 | train 0.2146/0.9480 | test 0.3311/0.9370\n",
            "Epoch 502 | train 0.2422/0.9404 | test 0.3807/0.9147\n",
            "Epoch 503 | train 0.2250/0.9456 | test 0.3491/0.9300\n",
            "Epoch 504 | train 0.2228/0.9443 | test 0.3581/0.9223\n",
            "Epoch 505 | train 0.2473/0.9346 | test 0.3906/0.9040\n",
            "Epoch 506 | train 0.2533/0.9309 | test 0.3559/0.9267\n",
            "Epoch 507 | train 0.2439/0.9333 | test 0.3281/0.9377\n",
            "Epoch 508 | train 0.2178/0.9441 | test 0.3404/0.9300\n",
            "Epoch 509 | train 0.2375/0.9387 | test 0.3629/0.9160\n",
            "Epoch 510 | train 0.2466/0.9305 | test 0.3273/0.9347\n",
            "Epoch 511 | train 0.2315/0.9407 | test 0.3617/0.9247\n",
            "Epoch 512 | train 0.2050/0.9510 | test 0.3718/0.9093\n",
            "Epoch 513 | train 0.2466/0.9304 | test 0.3966/0.9047\n",
            "Epoch 514 | train 0.2409/0.9343 | test 0.3100/0.9360\n",
            "Epoch 515 | train 0.3141/0.9012 | test 0.7302/0.8037\n",
            "Epoch 516 | train 0.3642/0.8871 | test 0.3654/0.9203\n",
            "Epoch 517 | train 0.2681/0.9219 | test 0.4559/0.8810\n",
            "Epoch 518 | train 0.2988/0.9133 | test 0.4162/0.8867\n",
            "Epoch 519 | train 0.3749/0.8816 | test 0.8646/0.7283\n",
            "Epoch 520 | train 3.9459/0.5269 | test 5.8199/0.3627\n",
            "Epoch 521 | train 4.8483/0.3750 | test 2.6138/0.5400\n",
            "Epoch 522 | train 1.3824/0.6528 | test 0.8264/0.7647\n",
            "Epoch 523 | train 0.5885/0.8080 | test 0.6344/0.8113\n",
            "Epoch 524 | train 0.5465/0.8157 | test 0.6949/0.8030\n",
            "Epoch 525 | train 0.5954/0.8145 | test 0.5001/0.8747\n",
            "Epoch 526 | train 0.3775/0.8802 | test 0.4444/0.8833\n",
            "Epoch 527 | train 0.6487/0.7943 | test 1.6853/0.6017\n",
            "Epoch 528 | train 0.9655/0.7047 | test 0.8522/0.7330\n",
            "Epoch 529 | train 0.7262/0.7679 | test 0.6583/0.8143\n",
            "Epoch 530 | train 0.3922/0.8730 | test 0.3767/0.9143\n",
            "Epoch 531 | train 0.2455/0.9344 | test 0.3295/0.9377\n",
            "Epoch 532 | train 0.2173/0.9471 | test 0.3205/0.9357\n",
            "Epoch 533 | train 0.2059/0.9499 | test 0.2917/0.9467\n",
            "Epoch 534 | train 0.1942/0.9536 | test 0.2952/0.9423\n",
            "Epoch 535 | train 0.2108/0.9465 | test 0.2949/0.9447\n",
            "Epoch 536 | train 0.2005/0.9505 | test 0.2973/0.9473\n",
            "Epoch 537 | train 0.1892/0.9565 | test 0.3095/0.9387\n",
            "Epoch 538 | train 0.1919/0.9556 | test 0.2895/0.9500\n",
            "Epoch 539 | train 0.2033/0.9478 | test 0.2807/0.9480\n",
            "Epoch 540 | train 0.1885/0.9579 | test 0.2913/0.9477\n",
            "Epoch 541 | train 0.1864/0.9572 | test 0.3003/0.9417\n",
            "Epoch 542 | train 0.1920/0.9538 | test 0.2940/0.9463\n",
            "Epoch 543 | train 0.1922/0.9527 | test 0.2985/0.9450\n",
            "Epoch 544 | train 0.1995/0.9493 | test 0.3080/0.9410\n",
            "Epoch 545 | train 0.1936/0.9543 | test 0.3119/0.9333\n",
            "Epoch 546 | train 0.2047/0.9455 | test 0.3083/0.9423\n",
            "Epoch 547 | train 0.1802/0.9592 | test 0.3126/0.9277\n",
            "Epoch 548 | train 0.1895/0.9535 | test 0.2797/0.9493\n",
            "Epoch 549 | train 0.1988/0.9503 | test 0.3149/0.9303\n",
            "Epoch 550 | train 0.1889/0.9555 | test 0.2762/0.9517\n",
            "Epoch 551 | train 0.2133/0.9430 | test 0.3620/0.9177\n",
            "Epoch 552 | train 0.2181/0.9419 | test 0.3027/0.9417\n",
            "Epoch 553 | train 0.2311/0.9355 | test 0.3867/0.9080\n",
            "Epoch 554 | train 0.2650/0.9227 | test 0.5227/0.8617\n",
            "Epoch 555 | train 0.3325/0.8922 | test 0.3962/0.8970\n",
            "Epoch 556 | train 0.3643/0.8762 | test 0.3555/0.9320\n",
            "Epoch 557 | train 0.2317/0.9390 | test 0.3110/0.9377\n",
            "Epoch 558 | train 0.1942/0.9509 | test 0.3149/0.9417\n",
            "Epoch 559 | train 0.1921/0.9521 | test 0.3090/0.9277\n",
            "Epoch 560 | train 0.1879/0.9540 | test 0.3075/0.9437\n",
            "Epoch 561 | train 0.2003/0.9493 | test 0.3301/0.9337\n",
            "Epoch 562 | train 1.4993/0.8116 | test 13.5998/0.2190\n",
            "Epoch 563 | train 8.1318/0.2853 | test 6.7021/0.2640\n",
            "Epoch 564 | train 3.5037/0.4454 | test 1.7756/0.5897\n",
            "Epoch 565 | train 1.0338/0.7178 | test 0.7164/0.8180\n",
            "Epoch 566 | train 0.4096/0.8722 | test 0.5208/0.8710\n",
            "Epoch 567 | train 0.2950/0.9147 | test 0.3578/0.9260\n",
            "Epoch 568 | train 0.2276/0.9408 | test 0.3685/0.9183\n",
            "Epoch 569 | train 0.2251/0.9414 | test 0.3294/0.9373\n",
            "Epoch 570 | train 0.2521/0.9301 | test 0.3199/0.9330\n",
            "Epoch 571 | train 0.2260/0.9408 | test 0.3387/0.9283\n",
            "Epoch 572 | train 0.2190/0.9428 | test 0.3170/0.9353\n",
            "Epoch 573 | train 0.2139/0.9439 | test 0.3413/0.9287\n",
            "Epoch 574 | train 0.1995/0.9517 | test 0.3543/0.9200\n",
            "Epoch 575 | train 0.2185/0.9401 | test 0.3431/0.9153\n",
            "Epoch 576 | train 0.2133/0.9428 | test 0.3286/0.9303\n",
            "Epoch 577 | train 0.2192/0.9422 | test 0.3227/0.9347\n",
            "Epoch 578 | train 0.1951/0.9483 | test 0.3034/0.9450\n",
            "Epoch 579 | train 0.1807/0.9607 | test 0.2779/0.9540\n",
            "Epoch 580 | train 0.1784/0.9586 | test 0.2924/0.9490\n",
            "Epoch 581 | train 0.1731/0.9593 | test 0.2839/0.9517\n",
            "Epoch 582 | train 0.2046/0.9475 | test 0.3201/0.9307\n",
            "Epoch 583 | train 0.2210/0.9393 | test 0.2979/0.9350\n",
            "Epoch 584 | train 0.1806/0.9567 | test 0.3058/0.9370\n",
            "Epoch 585 | train 0.1863/0.9536 | test 0.3035/0.9407\n",
            "Epoch 586 | train 0.2127/0.9433 | test 0.3019/0.9390\n",
            "Epoch 587 | train 0.1968/0.9489 | test 0.2935/0.9423\n",
            "Epoch 588 | train 0.2042/0.9456 | test 0.3071/0.9360\n",
            "Epoch 589 | train 0.1878/0.9517 | test 0.3288/0.9263\n",
            "Epoch 590 | train 0.3899/0.8682 | test 0.6153/0.8177\n",
            "Epoch 591 | train 0.5255/0.8220 | test 1.2205/0.6427\n",
            "Epoch 592 | train 4.4264/0.4055 | test 4.2963/0.4390\n",
            "Epoch 593 | train 3.6631/0.4440 | test 1.7742/0.5797\n",
            "Epoch 594 | train 1.0654/0.7040 | test 0.6331/0.8170\n",
            "Epoch 595 | train 0.5522/0.8171 | test 0.5323/0.8587\n",
            "Epoch 596 | train 0.6892/0.7790 | test 0.8440/0.7527\n",
            "Epoch 597 | train 0.8092/0.7368 | test 0.7988/0.7660\n",
            "Epoch 598 | train 1.0386/0.6885 | test 0.7321/0.7847\n",
            "Epoch 599 | train 0.3465/0.8927 | test 0.4159/0.9040\n",
            "Epoch 600 | train 0.2152/0.9463 | test 0.2945/0.9537\n",
            "Epoch 601 | train 0.1967/0.9515 | test 0.2880/0.9467\n",
            "Epoch 602 | train 0.1794/0.9594 | test 0.2769/0.9543\n",
            "Epoch 603 | train 0.1785/0.9582 | test 0.2938/0.9463\n",
            "Epoch 604 | train 0.1743/0.9573 | test 0.2837/0.9450\n",
            "Epoch 605 | train 0.1760/0.9599 | test 0.2756/0.9563\n",
            "Epoch 606 | train 0.1779/0.9578 | test 0.2838/0.9490\n",
            "Epoch 607 | train 0.1681/0.9617 | test 0.2756/0.9520\n",
            "Epoch 608 | train 0.1826/0.9563 | test 0.2778/0.9487\n",
            "Epoch 609 | train 0.1685/0.9583 | test 0.2587/0.9560\n",
            "Epoch 610 | train 0.1709/0.9604 | test 0.2790/0.9443\n",
            "Epoch 611 | train 0.1682/0.9599 | test 0.2919/0.9453\n",
            "Epoch 612 | train 0.1670/0.9598 | test 0.2848/0.9483\n",
            "Epoch 613 | train 0.1616/0.9621 | test 0.2540/0.9593\n",
            "Epoch 614 | train 0.1563/0.9655 | test 0.2621/0.9550\n",
            "Epoch 615 | train 0.1651/0.9616 | test 0.2746/0.9490\n",
            "Epoch 616 | train 0.1605/0.9632 | test 0.2686/0.9577\n",
            "Epoch 617 | train 0.1559/0.9648 | test 0.2592/0.9560\n",
            "Epoch 618 | train 0.1687/0.9594 | test 0.2803/0.9520\n",
            "Epoch 619 | train 0.1847/0.9513 | test 0.3081/0.9347\n",
            "Epoch 620 | train 0.1720/0.9591 | test 0.2638/0.9547\n",
            "Epoch 621 | train 0.1464/0.9695 | test 0.2503/0.9627\n",
            "Epoch 622 | train 0.1558/0.9650 | test 0.2627/0.9547\n",
            "Epoch 623 | train 0.1853/0.9523 | test 0.3725/0.9050\n",
            "Epoch 624 | train 0.2446/0.9289 | test 0.3731/0.9060\n",
            "Epoch 625 | train 0.2755/0.9144 | test 0.3779/0.9040\n",
            "Epoch 626 | train 0.2240/0.9361 | test 0.3490/0.9170\n",
            "Epoch 627 | train 0.1906/0.9506 | test 0.2682/0.9463\n",
            "Epoch 628 | train 0.1636/0.9603 | test 0.2834/0.9453\n",
            "Epoch 629 | train 0.1672/0.9602 | test 0.2989/0.9377\n",
            "Epoch 630 | train 0.1882/0.9487 | test 0.3151/0.9343\n",
            "Epoch 631 | train 0.2851/0.9135 | test 0.3987/0.9033\n",
            "Epoch 632 | train 5.8945/0.3944 | test 3.8162/0.4247\n",
            "Epoch 633 | train 2.6888/0.4992 | test 1.9298/0.5443\n",
            "Epoch 634 | train 1.1910/0.6838 | test 0.8144/0.7657\n",
            "Epoch 635 | train 0.3955/0.8764 | test 0.4425/0.8883\n",
            "Epoch 636 | train 0.2416/0.9319 | test 0.3363/0.9283\n",
            "Epoch 637 | train 0.2082/0.9447 | test 0.3106/0.9447\n",
            "Epoch 638 | train 0.1902/0.9494 | test 0.3147/0.9367\n",
            "Epoch 639 | train 0.1883/0.9492 | test 0.2898/0.9463\n",
            "Epoch 640 | train 0.1957/0.9465 | test 0.3499/0.9340\n",
            "Epoch 641 | train 0.1804/0.9536 | test 0.2723/0.9547\n",
            "Epoch 642 | train 0.1669/0.9606 | test 0.2895/0.9420\n",
            "Epoch 643 | train 0.1618/0.9619 | test 0.2765/0.9503\n",
            "Epoch 644 | train 0.1706/0.9544 | test 0.2746/0.9423\n",
            "Epoch 645 | train 0.1616/0.9603 | test 0.2762/0.9477\n",
            "Epoch 646 | train 0.1816/0.9543 | test 0.3119/0.9380\n",
            "Epoch 647 | train 0.1759/0.9543 | test 0.2813/0.9423\n",
            "Epoch 648 | train 0.1703/0.9572 | test 0.2875/0.9443\n",
            "Epoch 649 | train 0.1649/0.9597 | test 0.2886/0.9407\n",
            "Epoch 650 | train 0.1983/0.9466 | test 0.3386/0.9160\n",
            "Epoch 651 | train 0.1900/0.9490 | test 0.2907/0.9463\n",
            "Epoch 652 | train 0.1766/0.9547 | test 0.2936/0.9403\n",
            "Epoch 653 | train 0.2557/0.9238 | test 0.4282/0.9003\n",
            "Epoch 654 | train 0.3387/0.8864 | test 0.5002/0.8567\n",
            "Epoch 655 | train 0.2557/0.9203 | test 0.3812/0.9077\n",
            "Epoch 656 | train 0.1921/0.9459 | test 0.2914/0.9387\n",
            "Epoch 657 | train 0.1737/0.9555 | test 0.2817/0.9410\n",
            "Epoch 658 | train 0.1734/0.9548 | test 0.2710/0.9493\n",
            "Epoch 659 | train 0.1759/0.9544 | test 0.2890/0.9390\n",
            "Epoch 660 | train 0.1845/0.9493 | test 0.3239/0.9323\n",
            "Epoch 661 | train 0.1811/0.9512 | test 0.3316/0.9223\n",
            "Epoch 662 | train 0.1803/0.9505 | test 0.3212/0.9270\n",
            "Epoch 663 | train 0.2471/0.9233 | test 0.4060/0.8970\n",
            "Epoch 664 | train 0.2122/0.9366 | test 0.4070/0.8860\n",
            "Epoch 665 | train 0.2339/0.9274 | test 0.2864/0.9473\n",
            "Epoch 666 | train 0.1918/0.9469 | test 0.3382/0.9233\n",
            "Epoch 667 | train 0.1859/0.9488 | test 0.3305/0.9210\n",
            "Epoch 668 | train 0.1850/0.9501 | test 0.3058/0.9357\n",
            "Epoch 669 | train 0.1581/0.9593 | test 0.2674/0.9510\n",
            "Epoch 670 | train 0.1623/0.9579 | test 0.2751/0.9477\n",
            "Epoch 671 | train 0.1820/0.9514 | test 0.3848/0.8980\n",
            "Epoch 672 | train 0.2347/0.9277 | test 0.3031/0.9377\n",
            "Epoch 673 | train 0.1882/0.9467 | test 0.3366/0.9290\n",
            "Epoch 674 | train 0.2728/0.9110 | test 0.3901/0.8957\n",
            "Epoch 675 | train 17.6982/0.2702 | test 19.7062/0.1323\n",
            "Epoch 676 | train 22.3515/0.1655 | test 18.4097/0.1970\n",
            "Epoch 677 | train 8.9193/0.3270 | test 3.9049/0.4443\n",
            "Epoch 678 | train 2.0221/0.5982 | test 1.2200/0.6963\n",
            "Epoch 679 | train 0.7316/0.7747 | test 0.6070/0.8327\n",
            "Epoch 680 | train 0.3737/0.8876 | test 0.4535/0.8913\n",
            "Epoch 681 | train 0.2866/0.9155 | test 0.3720/0.9167\n",
            "Epoch 682 | train 0.2446/0.9300 | test 0.3458/0.9293\n",
            "Epoch 683 | train 0.2263/0.9372 | test 0.3392/0.9273\n",
            "Epoch 684 | train 0.2028/0.9503 | test 0.3379/0.9273\n",
            "Epoch 685 | train 0.2171/0.9398 | test 0.3609/0.9187\n",
            "Epoch 686 | train 0.2310/0.9310 | test 0.4371/0.8867\n",
            "Epoch 687 | train 0.2253/0.9340 | test 0.4122/0.9003\n",
            "Epoch 688 | train 0.2159/0.9411 | test 0.3481/0.9277\n",
            "Epoch 689 | train 0.2000/0.9459 | test 0.3174/0.9353\n",
            "Epoch 690 | train 0.1976/0.9469 | test 0.3409/0.9227\n",
            "Epoch 691 | train 0.2021/0.9458 | test 0.3177/0.9380\n",
            "Epoch 692 | train 0.1851/0.9520 | test 0.3148/0.9373\n",
            "Epoch 693 | train 0.1855/0.9497 | test 0.3488/0.9210\n",
            "Epoch 694 | train 0.2091/0.9378 | test 0.4067/0.8967\n",
            "Epoch 695 | train 0.2218/0.9336 | test 0.3301/0.9273\n",
            "Epoch 696 | train 0.1894/0.9495 | test 0.3174/0.9380\n",
            "Epoch 697 | train 0.1766/0.9537 | test 0.3000/0.9380\n",
            "Epoch 698 | train 0.1960/0.9437 | test 0.3089/0.9397\n",
            "Epoch 699 | train 0.1730/0.9563 | test 0.2859/0.9500\n",
            "Epoch 700 | train 0.1692/0.9573 | test 0.2922/0.9447\n",
            "Epoch 701 | train 0.1664/0.9577 | test 0.2736/0.9587\n",
            "Epoch 702 | train 0.1612/0.9598 | test 0.2977/0.9393\n",
            "Epoch 703 | train 0.1627/0.9567 | test 0.2926/0.9390\n",
            "Epoch 704 | train 0.1918/0.9456 | test 0.2785/0.9487\n",
            "Epoch 705 | train 0.1689/0.9547 | test 0.2867/0.9417\n",
            "Epoch 706 | train 0.1663/0.9570 | test 0.2814/0.9423\n",
            "Epoch 707 | train 0.1809/0.9500 | test 0.3213/0.9273\n",
            "Epoch 708 | train 0.1720/0.9550 | test 0.3027/0.9390\n",
            "Epoch 709 | train 0.1847/0.9503 | test 0.3260/0.9233\n",
            "Epoch 710 | train 0.1711/0.9542 | test 0.2952/0.9370\n",
            "Epoch 711 | train 0.1682/0.9547 | test 0.3453/0.9213\n",
            "Epoch 712 | train 0.1769/0.9517 | test 0.2990/0.9397\n",
            "Epoch 713 | train 0.1889/0.9467 | test 0.3038/0.9370\n",
            "Epoch 714 | train 0.1681/0.9539 | test 0.2824/0.9543\n",
            "Epoch 715 | train 0.2020/0.9384 | test 0.3624/0.9110\n",
            "Epoch 716 | train 0.2438/0.9233 | test 0.4446/0.8760\n",
            "Epoch 717 | train 0.2843/0.9067 | test 0.4537/0.8817\n",
            "Epoch 718 | train 0.3733/0.8782 | test 0.4407/0.8703\n",
            "Epoch 719 | train 0.2862/0.9025 | test 0.4576/0.8723\n",
            "Epoch 720 | train 0.3319/0.8876 | test 0.4817/0.8680\n",
            "Epoch 721 | train 0.4123/0.8587 | test 0.5591/0.8243\n",
            "Epoch 722 | train 0.2751/0.9122 | test 0.3614/0.9113\n",
            "Epoch 723 | train 0.1859/0.9446 | test 0.2754/0.9387\n",
            "Epoch 724 | train 0.1750/0.9533 | test 0.3172/0.9340\n",
            "Epoch 725 | train 0.1779/0.9501 | test 0.2606/0.9563\n",
            "Epoch 726 | train 0.1779/0.9522 | test 0.3448/0.9113\n",
            "Epoch 727 | train 0.2413/0.9240 | test 0.3236/0.9270\n",
            "Epoch 728 | train 0.2115/0.9346 | test 0.3277/0.9283\n",
            "Epoch 729 | train 0.1881/0.9466 | test 0.2712/0.9460\n",
            "Epoch 730 | train 0.1712/0.9514 | test 0.2785/0.9517\n",
            "Epoch 731 | train 0.2001/0.9421 | test 0.5087/0.8437\n",
            "Epoch 732 | train 7.8384/0.3363 | test 6.1020/0.3427\n",
            "Epoch 733 | train 3.9418/0.4412 | test 2.0040/0.5633\n",
            "Epoch 734 | train 0.8273/0.7592 | test 0.5435/0.8407\n",
            "Epoch 735 | train 0.3415/0.8944 | test 0.3701/0.9110\n",
            "Epoch 736 | train 0.2186/0.9388 | test 0.3401/0.9253\n",
            "Epoch 737 | train 0.1788/0.9513 | test 0.2978/0.9397\n",
            "Epoch 738 | train 0.1726/0.9547 | test 0.2919/0.9443\n",
            "Epoch 739 | train 0.1612/0.9576 | test 0.2610/0.9493\n",
            "Epoch 740 | train 0.1493/0.9643 | test 0.2668/0.9517\n",
            "Epoch 741 | train 0.1431/0.9662 | test 0.2473/0.9617\n",
            "Epoch 742 | train 0.1366/0.9684 | test 0.2543/0.9527\n",
            "Epoch 743 | train 0.1567/0.9587 | test 0.2949/0.9450\n",
            "Epoch 744 | train 0.1551/0.9618 | test 0.2709/0.9490\n",
            "Epoch 745 | train 0.1382/0.9669 | test 0.2933/0.9380\n",
            "Epoch 746 | train 0.1492/0.9627 | test 0.2452/0.9603\n",
            "Epoch 747 | train 0.1473/0.9612 | test 0.2589/0.9540\n",
            "Epoch 748 | train 0.1528/0.9604 | test 0.2638/0.9483\n",
            "Epoch 749 | train 0.1420/0.9653 | test 0.2654/0.9477\n",
            "Epoch 750 | train 0.1510/0.9611 | test 0.2715/0.9510\n",
            "Epoch 751 | train 0.1453/0.9631 | test 0.2965/0.9397\n",
            "Epoch 752 | train 0.1618/0.9569 | test 0.2394/0.9540\n",
            "Epoch 753 | train 0.1424/0.9650 | test 0.2918/0.9427\n",
            "Epoch 754 | train 0.1752/0.9517 | test 0.2853/0.9413\n",
            "Epoch 755 | train 0.1894/0.9423 | test 0.3096/0.9303\n",
            "Epoch 756 | train 0.1811/0.9483 | test 0.2751/0.9477\n",
            "Epoch 757 | train 0.1546/0.9586 | test 0.2741/0.9447\n",
            "Epoch 758 | train 0.1428/0.9638 | test 0.2609/0.9540\n",
            "Epoch 759 | train 0.1553/0.9585 | test 0.3518/0.9083\n",
            "Epoch 760 | train 0.2065/0.9378 | test 0.3025/0.9283\n",
            "Epoch 761 | train 0.1811/0.9492 | test 0.3006/0.9253\n",
            "Epoch 762 | train 0.2703/0.9121 | test 0.8074/0.7880\n",
            "Epoch 763 | train 4.3355/0.4968 | test 6.5021/0.3143\n",
            "Epoch 764 | train 3.3260/0.4567 | test 1.7434/0.5917\n",
            "Epoch 765 | train 0.7333/0.7788 | test 0.5452/0.8583\n",
            "Epoch 766 | train 0.3037/0.9054 | test 0.3446/0.9247\n",
            "Epoch 767 | train 0.1966/0.9466 | test 0.3059/0.9403\n",
            "Epoch 768 | train 0.1751/0.9537 | test 0.3200/0.9327\n",
            "Epoch 769 | train 0.1809/0.9482 | test 0.2681/0.9533\n",
            "Epoch 770 | train 0.1441/0.9642 | test 0.2598/0.9557\n",
            "Epoch 771 | train 0.1355/0.9681 | test 0.2476/0.9603\n",
            "Epoch 772 | train 0.1364/0.9663 | test 0.2771/0.9497\n",
            "Epoch 773 | train 0.1331/0.9682 | test 0.2474/0.9580\n",
            "Epoch 774 | train 0.1346/0.9682 | test 0.2846/0.9423\n",
            "Epoch 775 | train 0.1418/0.9636 | test 0.2891/0.9423\n",
            "Epoch 776 | train 0.1335/0.9666 | test 0.2385/0.9673\n",
            "Epoch 777 | train 0.1341/0.9676 | test 0.2573/0.9543\n",
            "Epoch 778 | train 0.1490/0.9619 | test 0.2455/0.9593\n",
            "Epoch 779 | train 0.1278/0.9708 | test 0.2349/0.9623\n",
            "Epoch 780 | train 0.1232/0.9709 | test 0.2534/0.9550\n",
            "Epoch 781 | train 0.1262/0.9718 | test 0.2488/0.9573\n",
            "Epoch 782 | train 0.1208/0.9718 | test 0.2392/0.9583\n",
            "Epoch 783 | train 0.1262/0.9688 | test 0.2643/0.9473\n",
            "Epoch 784 | train 0.1402/0.9634 | test 0.2650/0.9523\n",
            "Epoch 785 | train 0.1412/0.9627 | test 0.2666/0.9463\n",
            "Epoch 786 | train 0.1492/0.9621 | test 0.2498/0.9523\n",
            "Epoch 787 | train 0.1426/0.9643 | test 0.2388/0.9587\n",
            "Epoch 788 | train 0.1420/0.9669 | test 0.2560/0.9500\n",
            "Epoch 789 | train 0.1240/0.9723 | test 0.2706/0.9537\n",
            "Epoch 790 | train 0.1450/0.9632 | test 0.3653/0.9140\n",
            "Epoch 791 | train 1.3552/0.7399 | test 6.5318/0.3143\n",
            "Epoch 792 | train 4.7710/0.3964 | test 1.9258/0.6060\n",
            "Epoch 793 | train 0.9744/0.7324 | test 0.5992/0.8370\n",
            "Epoch 794 | train 0.4080/0.8648 | test 0.3719/0.9130\n",
            "Epoch 795 | train 0.2491/0.9236 | test 0.3068/0.9387\n",
            "Epoch 796 | train 0.1690/0.9553 | test 0.2591/0.9583\n",
            "Epoch 797 | train 0.1468/0.9628 | test 0.2761/0.9473\n",
            "Epoch 798 | train 0.1477/0.9637 | test 0.2927/0.9387\n",
            "Epoch 799 | train 0.1439/0.9603 | test 0.2870/0.9417\n",
            "Epoch 800 | train 0.1349/0.9680 | test 0.2521/0.9543\n",
            "Epoch 801 | train 0.1288/0.9707 | test 0.2690/0.9543\n",
            "Epoch 802 | train 0.1345/0.9667 | test 0.2496/0.9580\n",
            "Epoch 803 | train 0.1294/0.9698 | test 0.2747/0.9467\n",
            "Epoch 804 | train 0.1270/0.9692 | test 0.2394/0.9610\n",
            "Epoch 805 | train 0.1177/0.9732 | test 0.2339/0.9573\n",
            "Epoch 806 | train 0.1351/0.9641 | test 0.2511/0.9547\n",
            "Epoch 807 | train 0.1390/0.9633 | test 0.2493/0.9567\n",
            "Epoch 808 | train 0.1284/0.9679 | test 0.2809/0.9453\n",
            "Epoch 809 | train 0.1410/0.9637 | test 0.2747/0.9463\n",
            "Epoch 810 | train 0.1457/0.9602 | test 0.2630/0.9477\n",
            "Epoch 811 | train 0.1483/0.9594 | test 0.2850/0.9463\n",
            "Epoch 812 | train 0.1342/0.9697 | test 0.2551/0.9507\n",
            "Epoch 813 | train 0.1418/0.9601 | test 0.2524/0.9567\n",
            "Epoch 814 | train 0.1497/0.9598 | test 0.2678/0.9437\n",
            "Epoch 815 | train 0.1351/0.9661 | test 0.2748/0.9483\n",
            "Epoch 816 | train 0.1352/0.9657 | test 0.2670/0.9520\n",
            "Epoch 817 | train 0.1352/0.9623 | test 0.2372/0.9627\n",
            "Epoch 818 | train 0.1218/0.9705 | test 0.2226/0.9627\n",
            "Epoch 819 | train 0.1151/0.9703 | test 0.2652/0.9540\n",
            "Epoch 820 | train 1.5745/0.7717 | test 7.4813/0.2833\n",
            "Epoch 821 | train 6.2817/0.3367 | test 3.0644/0.4857\n",
            "Epoch 822 | train 1.4322/0.6593 | test 1.0632/0.7223\n",
            "Epoch 823 | train 0.5089/0.8433 | test 0.5008/0.8703\n",
            "Epoch 824 | train 0.2761/0.9136 | test 0.3449/0.9210\n",
            "Epoch 825 | train 0.2030/0.9397 | test 0.3092/0.9407\n",
            "Epoch 826 | train 0.1674/0.9546 | test 0.2646/0.9537\n",
            "Epoch 827 | train 0.1399/0.9655 | test 0.2701/0.9537\n",
            "Epoch 828 | train 0.1465/0.9606 | test 0.2671/0.9490\n",
            "Epoch 829 | train 0.1392/0.9633 | test 0.2596/0.9593\n",
            "Epoch 830 | train 0.1339/0.9666 | test 0.2555/0.9603\n",
            "Epoch 831 | train 0.1286/0.9695 | test 0.2411/0.9623\n",
            "Epoch 832 | train 0.1208/0.9697 | test 0.2758/0.9400\n",
            "Epoch 833 | train 0.1354/0.9644 | test 0.2617/0.9517\n",
            "Epoch 834 | train 0.1276/0.9685 | test 0.2372/0.9670\n",
            "Epoch 835 | train 0.1124/0.9754 | test 0.2262/0.9633\n",
            "Epoch 836 | train 0.1272/0.9665 | test 0.2817/0.9380\n",
            "Epoch 837 | train 0.1347/0.9647 | test 0.2657/0.9467\n",
            "Epoch 838 | train 0.1414/0.9618 | test 0.2596/0.9497\n",
            "Epoch 839 | train 0.1196/0.9717 | test 0.2460/0.9580\n",
            "Epoch 840 | train 0.1175/0.9722 | test 0.2468/0.9567\n",
            "Epoch 841 | train 0.1283/0.9660 | test 0.2439/0.9613\n",
            "Epoch 842 | train 0.1261/0.9668 | test 0.2357/0.9620\n",
            "Epoch 843 | train 0.1186/0.9713 | test 0.2401/0.9537\n",
            "Epoch 844 | train 0.1256/0.9689 | test 0.2531/0.9523\n",
            "Epoch 845 | train 0.1232/0.9692 | test 0.2461/0.9523\n",
            "Epoch 846 | train 0.1258/0.9683 | test 0.2484/0.9610\n",
            "Epoch 847 | train 0.1515/0.9570 | test 0.3280/0.9290\n",
            "Epoch 848 | train 0.1456/0.9622 | test 0.2501/0.9550\n",
            "Epoch 849 | train 0.1182/0.9706 | test 0.2417/0.9627\n",
            "Epoch 850 | train 0.1329/0.9663 | test 0.2818/0.9417\n",
            "Epoch 851 | train 0.1621/0.9501 | test 0.2994/0.9303\n",
            "Epoch 852 | train 0.1593/0.9527 | test 0.3049/0.9367\n",
            "Epoch 853 | train 0.2055/0.9324 | test 0.4796/0.8733\n",
            "Epoch 854 | train 0.3099/0.8989 | test 0.7418/0.8143\n",
            "Epoch 855 | train 5.3002/0.5156 | test 10.8348/0.2327\n",
            "Epoch 856 | train 5.9791/0.3666 | test 7.2106/0.3343\n",
            "Epoch 857 | train 2.5620/0.5655 | test 1.2153/0.6933\n",
            "Epoch 858 | train 0.6568/0.8089 | test 0.5557/0.8610\n",
            "Epoch 859 | train 0.2961/0.9093 | test 0.3485/0.9167\n",
            "Epoch 860 | train 0.1865/0.9479 | test 0.3101/0.9307\n",
            "Epoch 861 | train 0.1570/0.9561 | test 0.2952/0.9387\n",
            "Epoch 862 | train 0.1561/0.9573 | test 0.2728/0.9457\n",
            "Epoch 863 | train 0.1673/0.9517 | test 0.2971/0.9390\n",
            "Epoch 864 | train 0.1390/0.9643 | test 0.2599/0.9503\n",
            "Epoch 865 | train 0.1328/0.9674 | test 0.2624/0.9487\n",
            "Epoch 866 | train 0.1398/0.9633 | test 0.2525/0.9583\n",
            "Epoch 867 | train 0.1310/0.9673 | test 0.2697/0.9503\n",
            "Epoch 868 | train 0.1237/0.9688 | test 0.2443/0.9553\n",
            "Epoch 869 | train 0.1205/0.9716 | test 0.2608/0.9503\n",
            "Epoch 870 | train 0.1293/0.9671 | test 0.2304/0.9637\n",
            "Epoch 871 | train 0.1196/0.9719 | test 0.2504/0.9537\n",
            "Epoch 872 | train 0.1204/0.9706 | test 0.2783/0.9367\n",
            "Epoch 873 | train 0.1197/0.9728 | test 0.2272/0.9647\n",
            "Epoch 874 | train 0.1063/0.9767 | test 0.2537/0.9580\n",
            "Epoch 875 | train 0.1271/0.9664 | test 0.2460/0.9547\n",
            "Epoch 876 | train 0.1170/0.9709 | test 0.2251/0.9653\n",
            "Epoch 877 | train 0.1133/0.9728 | test 0.2555/0.9530\n",
            "Epoch 878 | train 0.1664/0.9500 | test 0.2792/0.9457\n",
            "Epoch 879 | train 0.1906/0.9402 | test 0.4305/0.8937\n",
            "Epoch 880 | train 0.5451/0.8212 | test 0.6541/0.8097\n",
            "Epoch 881 | train 0.4684/0.8448 | test 0.6039/0.8250\n",
            "Epoch 882 | train 0.3660/0.8748 | test 0.6704/0.8140\n",
            "Epoch 883 | train 0.5714/0.8153 | test 0.6743/0.7960\n",
            "Epoch 884 | train 0.6259/0.7978 | test 1.0024/0.7357\n",
            "Epoch 885 | train 0.6354/0.7955 | test 0.6974/0.7887\n",
            "Epoch 886 | train 0.6126/0.7958 | test 0.7207/0.7883\n",
            "Epoch 887 | train 0.2959/0.8989 | test 0.3149/0.9260\n",
            "Epoch 888 | train 0.1845/0.9459 | test 0.2988/0.9370\n",
            "Epoch 889 | train 0.1543/0.9580 | test 0.2496/0.9493\n",
            "Epoch 890 | train 0.1369/0.9627 | test 0.2268/0.9660\n",
            "Epoch 891 | train 0.1190/0.9727 | test 0.2312/0.9583\n",
            "Epoch 892 | train 0.1231/0.9687 | test 0.2299/0.9640\n",
            "Epoch 893 | train 0.1091/0.9753 | test 0.2438/0.9463\n",
            "Epoch 894 | train 0.1153/0.9712 | test 0.2533/0.9453\n",
            "Epoch 895 | train 0.1134/0.9725 | test 0.2818/0.9410\n",
            "Epoch 896 | train 0.1358/0.9615 | test 0.2631/0.9467\n",
            "Epoch 897 | train 0.1267/0.9661 | test 0.2264/0.9657\n",
            "Epoch 898 | train 0.1162/0.9708 | test 0.2177/0.9687\n",
            "Epoch 899 | train 0.1038/0.9762 | test 0.2262/0.9637\n",
            "Epoch 900 | train 0.1006/0.9763 | test 0.2323/0.9593\n",
            "Epoch 901 | train 0.1054/0.9742 | test 0.2346/0.9597\n",
            "Epoch 902 | train 0.1010/0.9782 | test 0.2203/0.9663\n",
            "Epoch 903 | train 0.0960/0.9791 | test 0.2198/0.9623\n",
            "Epoch 904 | train 0.1058/0.9734 | test 0.2301/0.9570\n",
            "Epoch 905 | train 0.1110/0.9720 | test 0.2358/0.9577\n",
            "Epoch 906 | train 0.1146/0.9719 | test 0.2493/0.9510\n",
            "Epoch 907 | train 0.1096/0.9739 | test 0.2204/0.9593\n",
            "Epoch 908 | train 0.1196/0.9653 | test 0.2781/0.9390\n",
            "Epoch 909 | train 0.1327/0.9637 | test 0.2421/0.9603\n",
            "Epoch 910 | train 0.1476/0.9537 | test 0.2720/0.9487\n",
            "Epoch 911 | train 4.9172/0.6132 | test 11.5374/0.2053\n",
            "Epoch 912 | train 7.3710/0.3151 | test 5.2440/0.3960\n",
            "Epoch 913 | train 3.1230/0.5113 | test 1.8302/0.6320\n",
            "Epoch 914 | train 0.8734/0.7682 | test 0.5246/0.8567\n",
            "Epoch 915 | train 0.2840/0.9127 | test 0.3412/0.9270\n",
            "Epoch 916 | train 0.2082/0.9356 | test 0.2954/0.9470\n",
            "Epoch 917 | train 0.1730/0.9514 | test 0.2696/0.9500\n",
            "Epoch 918 | train 0.1503/0.9595 | test 0.2830/0.9513\n",
            "Epoch 919 | train 0.1391/0.9644 | test 0.2591/0.9550\n",
            "Epoch 920 | train 0.1218/0.9685 | test 0.2370/0.9623\n",
            "Epoch 921 | train 0.1135/0.9744 | test 0.2404/0.9607\n",
            "Epoch 922 | train 0.1224/0.9688 | test 0.2463/0.9577\n",
            "Epoch 923 | train 0.1293/0.9665 | test 0.2566/0.9503\n",
            "Epoch 924 | train 0.1276/0.9666 | test 0.2465/0.9610\n",
            "Epoch 925 | train 0.1198/0.9704 | test 0.2478/0.9557\n",
            "Epoch 926 | train 0.1154/0.9715 | test 0.2446/0.9587\n",
            "Epoch 927 | train 0.1144/0.9706 | test 0.2441/0.9603\n",
            "Epoch 928 | train 0.1158/0.9706 | test 0.2576/0.9510\n",
            "Epoch 929 | train 0.1167/0.9708 | test 0.2337/0.9653\n",
            "Epoch 930 | train 0.1078/0.9754 | test 0.2231/0.9690\n",
            "Epoch 931 | train 0.1103/0.9718 | test 0.2392/0.9593\n",
            "Epoch 932 | train 0.1153/0.9708 | test 0.2444/0.9520\n",
            "Epoch 933 | train 0.1099/0.9728 | test 0.2319/0.9657\n",
            "Epoch 934 | train 0.1038/0.9748 | test 0.2408/0.9593\n",
            "Epoch 935 | train 0.1131/0.9718 | test 0.2402/0.9650\n",
            "Epoch 936 | train 0.1117/0.9708 | test 0.2546/0.9550\n",
            "Epoch 937 | train 0.1030/0.9771 | test 0.2190/0.9667\n",
            "Epoch 938 | train 0.1001/0.9776 | test 0.2220/0.9663\n",
            "Epoch 939 | train 0.1037/0.9757 | test 0.2393/0.9667\n",
            "Epoch 940 | train 0.2873/0.9153 | test 1.4816/0.6677\n",
            "Epoch 941 | train 1.4236/0.6589 | test 1.1989/0.6973\n",
            "Epoch 942 | train 0.6255/0.8134 | test 0.3863/0.8993\n",
            "Epoch 943 | train 0.2219/0.9312 | test 0.3173/0.9320\n",
            "Epoch 944 | train 0.1692/0.9474 | test 0.2716/0.9450\n",
            "Epoch 945 | train 0.1241/0.9663 | test 0.2483/0.9593\n",
            "Epoch 946 | train 0.1430/0.9601 | test 0.2386/0.9597\n",
            "Epoch 947 | train 0.1289/0.9642 | test 0.2452/0.9537\n",
            "Epoch 948 | train 0.1194/0.9677 | test 0.2460/0.9477\n",
            "Epoch 949 | train 0.1338/0.9651 | test 0.2356/0.9540\n",
            "Epoch 950 | train 0.1154/0.9710 | test 0.2417/0.9580\n",
            "Epoch 951 | train 0.1076/0.9708 | test 0.2299/0.9647\n",
            "Epoch 952 | train 0.1092/0.9718 | test 0.2407/0.9457\n",
            "Epoch 953 | train 0.1246/0.9652 | test 0.2687/0.9480\n",
            "Epoch 954 | train 0.1306/0.9610 | test 0.2670/0.9473\n",
            "Epoch 955 | train 0.1414/0.9572 | test 0.2724/0.9490\n",
            "Epoch 956 | train 0.1367/0.9613 | test 0.2526/0.9523\n",
            "Epoch 957 | train 0.1151/0.9702 | test 0.2112/0.9677\n",
            "Epoch 958 | train 0.0942/0.9777 | test 0.2244/0.9603\n",
            "Epoch 959 | train 0.1165/0.9692 | test 0.2890/0.9447\n",
            "Epoch 960 | train 0.1236/0.9667 | test 0.2510/0.9527\n",
            "Epoch 961 | train 0.1200/0.9664 | test 0.2230/0.9637\n",
            "Epoch 962 | train 0.1045/0.9725 | test 0.2677/0.9380\n",
            "Epoch 963 | train 0.0984/0.9761 | test 0.2066/0.9670\n",
            "Epoch 964 | train 0.1079/0.9691 | test 0.2856/0.9297\n",
            "Epoch 965 | train 0.1414/0.9561 | test 0.2869/0.9360\n",
            "Epoch 966 | train 0.1566/0.9517 | test 0.2767/0.9427\n",
            "Epoch 967 | train 0.1999/0.9353 | test 0.3334/0.9207\n",
            "Epoch 968 | train 0.1650/0.9481 | test 0.3058/0.9343\n",
            "Epoch 969 | train 0.1435/0.9567 | test 0.3180/0.9203\n",
            "Epoch 970 | train 1.2946/0.8377 | test 14.1364/0.2213\n",
            "Epoch 971 | train 12.7567/0.2386 | test 11.0586/0.2290\n",
            "Epoch 972 | train 6.2993/0.3777 | test 3.3455/0.4590\n",
            "Epoch 973 | train 2.3172/0.5741 | test 1.4585/0.6683\n",
            "Epoch 974 | train 0.6189/0.8179 | test 0.4654/0.8823\n",
            "Epoch 975 | train 0.2921/0.9088 | test 0.4019/0.9073\n",
            "Epoch 976 | train 0.2250/0.9293 | test 0.3572/0.9250\n",
            "Epoch 977 | train 0.1977/0.9398 | test 0.3057/0.9413\n",
            "Epoch 978 | train 0.1519/0.9596 | test 0.2663/0.9503\n",
            "Epoch 979 | train 0.1364/0.9623 | test 0.2905/0.9467\n",
            "Epoch 980 | train 0.1353/0.9640 | test 0.2664/0.9553\n",
            "Epoch 981 | train 0.1276/0.9643 | test 0.2599/0.9570\n",
            "Epoch 982 | train 0.1322/0.9627 | test 0.2546/0.9577\n",
            "Epoch 983 | train 0.1313/0.9623 | test 0.2662/0.9523\n",
            "Epoch 984 | train 0.1264/0.9642 | test 0.2484/0.9583\n",
            "Epoch 985 | train 0.1160/0.9687 | test 0.2505/0.9567\n",
            "Epoch 986 | train 0.1222/0.9683 | test 0.2714/0.9467\n",
            "Epoch 987 | train 0.1235/0.9664 | test 0.2616/0.9473\n",
            "Epoch 988 | train 0.1419/0.9584 | test 0.3913/0.9090\n",
            "Epoch 989 | train 0.1464/0.9563 | test 0.2904/0.9337\n",
            "Epoch 990 | train 0.1222/0.9680 | test 0.2371/0.9617\n",
            "Epoch 991 | train 0.1084/0.9726 | test 0.2419/0.9653\n",
            "Epoch 992 | train 0.1057/0.9722 | test 0.2494/0.9530\n",
            "Epoch 993 | train 0.1075/0.9732 | test 0.2603/0.9563\n",
            "Epoch 994 | train 0.1077/0.9722 | test 0.2703/0.9473\n",
            "Epoch 995 | train 0.1134/0.9683 | test 0.2498/0.9487\n",
            "Epoch 996 | train 0.1178/0.9697 | test 0.2426/0.9593\n",
            "Epoch 997 | train 0.1241/0.9633 | test 0.2468/0.9587\n",
            "Epoch 998 | train 0.1113/0.9687 | test 0.2622/0.9463\n",
            "Epoch 999 | train 0.1495/0.9535 | test 0.2644/0.9460\n",
            "Epoch 1000 | train 0.1380/0.9603 | test 0.2281/0.9673\n",
            "\n",
            "Test accuracy: 0.9673333333333334\n",
            "\n",
            "Per-class report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9899    0.9800    0.9849       100\n",
            "           1     0.9700    0.9700    0.9700       100\n",
            "           2     0.9894    0.9300    0.9588       100\n",
            "           3     0.9800    0.9800    0.9800       100\n",
            "           4     1.0000    0.9700    0.9848       100\n",
            "           5     1.0000    0.9900    0.9950       100\n",
            "           6     0.9434    1.0000    0.9709       100\n",
            "           7     0.9423    0.9800    0.9608       100\n",
            "           8     0.9798    0.9700    0.9749       100\n",
            "           9     0.9792    0.9400    0.9592       100\n",
            "          10     0.9604    0.9700    0.9652       100\n",
            "          11     1.0000    1.0000    1.0000       100\n",
            "          12     1.0000    0.9600    0.9796       100\n",
            "          13     1.0000    0.8600    0.9247       100\n",
            "          14     0.7619    0.9600    0.8496       100\n",
            "          15     0.9798    0.9700    0.9749       100\n",
            "          16     0.9898    0.9700    0.9798       100\n",
            "          17     0.9524    1.0000    0.9756       100\n",
            "          18     0.9091    0.8000    0.8511       100\n",
            "          19     0.9804    1.0000    0.9901       100\n",
            "          20     1.0000    0.9800    0.9899       100\n",
            "          21     1.0000    0.9700    0.9848       100\n",
            "          22     1.0000    0.9900    0.9950       100\n",
            "          23     0.9091    1.0000    0.9524       100\n",
            "          24     0.9700    0.9700    0.9700       100\n",
            "          25     1.0000    0.9900    0.9950       100\n",
            "          26     0.9423    0.9800    0.9608       100\n",
            "          27     0.9798    0.9700    0.9749       100\n",
            "          28     0.9800    0.9800    0.9800       100\n",
            "          29     1.0000    0.9900    0.9950       100\n",
            "\n",
            "    accuracy                         0.9673      3000\n",
            "   macro avg     0.9696    0.9673    0.9676      3000\n",
            "weighted avg     0.9696    0.9673    0.9676      3000\n",
            "\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            " [[ 98   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    1   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0  97   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   3   0   0   0   0   0   0]\n",
            " [  0   0  93   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4\n",
            "    1   0   0   0   0   2   0   0   0   0   0   0]\n",
            " [  0   0   0  98   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   2   0]\n",
            " [  0   0   0   0  97   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   3   0   0   0]\n",
            " [  0   0   0   0   0  99   0   1   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 100   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   2  98   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  97   0   3   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   1   4   0  94   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   1   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  97   0   0   0   2   0   1   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 100   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  96   0   0   0   0   0\n",
            "    0   0   0   0   0   2   2   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  86  11   0   0   0\n",
            "    2   0   0   0   0   0   1   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  96   0   0   0\n",
            "    4   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   3   0   0   0   0   0   0   0   0   0   0   0   0   0  97   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0  97   0\n",
            "    0   0   0   0   0   0   0   0   0   1   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 100\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0  17   2   0   0\n",
            "   80   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0 100   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0  98   0   0   0   0   0   2   0   0   0]\n",
            " [  0   0   0   0   0   0   3   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0  97   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0  99   0   0   0   0   1   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0 100   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
            "    0   0   0   0   0   2  97   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   1   0  99   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0  98   0   0   0]\n",
            " [  0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
            "    0   2   0   0   0   0   0   0   0  97   0   0]\n",
            " [  0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0  98   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0  99]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, json\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# === load your trained model weights ===\n",
        "CKPT_PATH = \"/content/drive/MyDrive/code_data25/Basu/model_save/best_router_mlp_80_20.pth\"  # <-- change if needed\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# If you saved a dict with {\"model\": state_dict, ...}\n",
        "ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
        "state_dict = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
        "\n",
        "model.to(DEVICE)\n",
        "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "print(\"Loaded weights. Missing keys:\", missing, \"| Unexpected keys:\", unexpected)\n",
        "\n",
        "# write label_map if present\n",
        "if isinstance(ckpt, dict) and \"label_map\" in ckpt:\n",
        "    with open(\"label_map.json\", \"w\") as f:\n",
        "        json.dump(ckpt[\"label_map\"], f, indent=2)\n",
        "\n",
        "# === evaluation + saving ===\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        pr = model(xb.to(DEVICE)).argmax(1).cpu().numpy()\n",
        "        y_pred.append(pr); y_true.append(yb.numpy())\n",
        "y_true = np.concatenate(y_true); y_pred = np.concatenate(y_pred)\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "rep = classification_report(y_true, y_pred, digits=4)\n",
        "cm  = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/code_data25/Basu/save_results/test_accuracy.txt\", \"w\") as f:\n",
        "    f.write(f\"{acc:.6f}\\n\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/code_data25/Basu/save_results/classification_report.txt\", \"w\") as f:\n",
        "    f.write(rep + \"\\n\")\n",
        "\n",
        "np.savetxt(\"/content/drive/MyDrive/code_data25/Basu/save_results/confusion_matrix.txt\", cm, fmt=\"%d\")\n",
        "\n",
        "np.savetxt(\n",
        "    \"/content/drive/MyDrive/code_data25/Basu/save_results/predictions.csv\",\n",
        "    np.column_stack([y_true, y_pred]),\n",
        "    fmt=\"%d\",\n",
        "    delimiter=\",\",\n",
        "    header=\"y_true,y_pred\",\n",
        "    comments=\"\"\n",
        ")\n",
        "\n",
        "df = pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred})\n",
        "per_class_acc = df.groupby(\"y_true\").apply(lambda g: float((g.y_true == g.y_pred).mean()))\n",
        "per_class_acc.to_csv(\"/content/drive/MyDrive/code_data25/Basu/save_results/per_class_accuracy.csv\", header=[\"accuracy\"])\n",
        "\n",
        "print(\"Saved: test_accuracy.txt, classification_report.txt, confusion_matrix.txt, predictions.csv, per_class_accuracy.csv\")\n",
        "\n",
        "# ========= single-sample latency + GFLOPs report =========\n",
        "import os, time, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/code_data25/Basu/save_results\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# 1) Take one real sample from your test loader\n",
        "model.eval()\n",
        "xb_one, _ = next(iter(test_loader))          # uses your real preprocessing pipeline\n",
        "xb_one = xb_one[:1].to(DEVICE)               # shape [1, D]\n",
        "\n",
        "# 2) Accurate timing (GPU-aware)\n",
        "def time_inference(model, x, warmup=30, repeats=200):\n",
        "    with torch.no_grad():\n",
        "        # warmup (not measured)\n",
        "        for _ in range(warmup):\n",
        "            _ = model(x)\n",
        "            if x.is_cuda:\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "        times = []\n",
        "        if x.is_cuda:\n",
        "            start = torch.cuda.Event(enable_timing=True)\n",
        "            end   = torch.cuda.Event(enable_timing=True)\n",
        "            for _ in range(repeats):\n",
        "                start.record(); _ = model(x); end.record()\n",
        "                torch.cuda.synchronize()\n",
        "                times.append(start.elapsed_time(end) / 1000.0)  # seconds\n",
        "        else:\n",
        "            for _ in range(repeats):\n",
        "                t0 = time.perf_counter(); _ = model(x); t1 = time.perf_counter()\n",
        "                times.append(t1 - t0)\n",
        "\n",
        "    times = np.array(times, dtype=np.float64)\n",
        "    return {\n",
        "        \"device\": \"cuda\" if x.is_cuda else \"cpu\",\n",
        "        \"batch_size\": int(x.shape[0]),\n",
        "        \"runs\": int(repeats),\n",
        "        \"mean_s\": float(times.mean()),\n",
        "        \"median_s\": float(np.median(times)),\n",
        "        \"p90_s\": float(np.percentile(times, 90)),\n",
        "        \"p95_s\": float(np.percentile(times, 95)),\n",
        "        \"min_s\": float(times.min()),\n",
        "        \"max_s\": float(times.max()),\n",
        "    }\n",
        "\n",
        "# 3) FLOPs for MLP (sum over Linear layers): 1 multiply + 1 add = 2 FLOPs\n",
        "def mlp_flops_per_sample(model):\n",
        "    flops = 0\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            flops += 2 * m.in_features * m.out_features\n",
        "    return flops  # per-sample\n",
        "\n",
        "def num_params(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "timing = time_inference(model, xb_one, warmup=30, repeats=200)\n",
        "flops  = mlp_flops_per_sample(model)\n",
        "report = {\n",
        "    \"device\": timing[\"device\"],\n",
        "    \"batch_size\": timing[\"batch_size\"],\n",
        "    \"runs\": timing[\"runs\"],\n",
        "    \"per_sample_time_s_mean\": timing[\"mean_s\"],\n",
        "    \"per_sample_time_s_median\": timing[\"median_s\"],\n",
        "    \"per_sample_time_s_p90\": timing[\"p90_s\"],\n",
        "    \"per_sample_time_s_p95\": timing[\"p95_s\"],\n",
        "    \"per_sample_time_s_min\": timing[\"min_s\"],\n",
        "    \"throughput_samples_per_s_mean\": (1.0 / timing[\"mean_s\"]) if timing[\"mean_s\"] > 0 else None,\n",
        "    \"params_total\": int(num_params(model)),\n",
        "    \"flops_per_sample\": int(flops),\n",
        "    \"gflops_per_sample\": float(flops / 1e9),\n",
        "}\n",
        "\n",
        "print(json.dumps(report, indent=2))\n",
        "Path(f\"{OUT_DIR}/inference_profile.json\").write_text(json.dumps(report, indent=2))\n",
        "print(f\"Saved {OUT_DIR}/inference_profile.json\")\n",
        "\n",
        "# ========= per-class bar, ROC curves, and t-SNE (save PNGs) =========\n",
        "import os, json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/code_data25/Basu/save_results\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- A) Per-class accuracy bar plot ----------\n",
        "df_eval = pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred})\n",
        "per_class_acc = df_eval.groupby(\"y_true\").apply(lambda g: float((g.y_true == g.y_pred).mean()))\n",
        "per_class_acc = per_class_acc.sort_values(ascending=True)  # ascending so worst classes at top\n",
        "\n",
        "plt.figure(figsize=(7, max(4, 0.22 * len(per_class_acc))), dpi=400)\n",
        "plt.barh(per_class_acc.index.astype(str), per_class_acc.values)\n",
        "plt.xlabel(\"Per-class Accuracy\")\n",
        "plt.ylabel(\"Class ID\")\n",
        "plt.title(\"Per-class Accuracy (Sorted)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUT_DIR}/per_class_accuracy_bar.png\", bbox_inches=\"tight\", facecolor=\"white\")\n",
        "plt.close()\n",
        "\n",
        "# ---------- B) ROC curves (micro/macro + OvR) ----------\n",
        "# We need class probabilities.\n",
        "model.eval()\n",
        "all_scores, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        logits = model(xb.to(DEVICE))\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "        all_scores.append(probs)\n",
        "        all_labels.append(yb.numpy())\n",
        "y_score = np.vstack(all_scores)  # shape [N, C]\n",
        "y_true_arr = np.concatenate(all_labels)  # shape [N]\n",
        "classes = np.unique(y_true_arr)\n",
        "n_classes = classes.size\n",
        "\n",
        "# Binarize labels for multi-class ROC\n",
        "y_true_bin = label_binarize(y_true_arr, classes=classes)  # shape [N, C]\n",
        "\n",
        "# Compute micro, macro, and OvR ROC\n",
        "fpr, tpr, roc_auc = {}, {}, {}\n",
        "# OvR for each class (columns aligned with 'classes')\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# micro-average\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# macro-average\n",
        "# compute average of the interpolated TPRs\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i], left=0, right=1)\n",
        "mean_tpr /= n_classes\n",
        "fpr[\"macro\"], tpr[\"macro\"] = all_fpr, mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Plot\n",
        "fig = plt.figure(figsize=(7, 6), dpi=400, facecolor=\"white\")\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1)\n",
        "\n",
        "# Emphasize micro & macro\n",
        "ax.plot(fpr[\"micro\"], tpr[\"micro\"], linewidth=2, label=f\"micro-average (AUC = {roc_auc['micro']:.3f})\")\n",
        "ax.plot(fpr[\"macro\"], tpr[\"macro\"], linewidth=2, label=f\"macro-average (AUC = {roc_auc['macro']:.3f})\")\n",
        "\n",
        "# Plot a few one-vs-rest class curves (to avoid clutter if many classes)\n",
        "MAX_OVR = 8\n",
        "pick = np.linspace(0, n_classes-1, min(n_classes, MAX_OVR), dtype=int)\n",
        "for i in pick:\n",
        "    ax.plot(fpr[i], tpr[i], linewidth=1, label=f\"class {classes[i]} (AUC = {roc_auc[i]:.3f})\")\n",
        "\n",
        "ax.set_xlabel(\"False Positive Rate\")\n",
        "ax.set_ylabel(\"True Positive Rate\")\n",
        "ax.set_title(\"ROC Curves: micro, macro, and One-vs-Rest\")\n",
        "ax.legend(loc=\"lower right\", fontsize=7)\n",
        "fig.tight_layout()\n",
        "fig.savefig(f\"{OUT_DIR}/roc_curves_micro_macro_onevsrest.png\", bbox_inches=\"tight\", facecolor=\"white\")\n",
        "plt.close(fig)\n",
        "\n",
        "# ---------- C) t-SNE on penultimate features (fallback to logits if needed) ----------\n",
        "# We’ll try to grab the penultimate activations by registering a forward hook on the last Linear layer’s input.\n",
        "# If that fails, we’ll use logits as features.\n",
        "last_linear = None\n",
        "for m in model.modules():\n",
        "    if isinstance(m, nn.Linear):\n",
        "        last_linear = m\n",
        "# Container to hold features from the forward hook\n",
        "_feats = []\n",
        "def hook_fn(module, inputs, output):\n",
        "    # inputs is a tuple; we want the input activations to this linear layer (penultimate features)\n",
        "    _feats.append(inputs[0].detach().cpu())\n",
        "\n",
        "h = None\n",
        "if last_linear is not None:\n",
        "    h = last_linear.register_forward_hook(hook_fn)\n",
        "\n",
        "# Run forward passes to collect features\n",
        "model.eval()\n",
        "feat_list, lab_list = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        _ = model(xb.to(DEVICE))\n",
        "        if _feats:\n",
        "            feat_list.append(_feats[-1])  # latest batch’s features\n",
        "        else:\n",
        "            # Fallback to logits if hook didn't fire\n",
        "            feat_list.append(_.detach().cpu())\n",
        "        lab_list.append(yb)\n",
        "\n",
        "# Cleanup hook\n",
        "if h is not None:\n",
        "    h.remove()\n",
        "\n",
        "feats = torch.cat(feat_list, dim=0).cpu().numpy()\n",
        "labs  = torch.cat(lab_list, dim=0).cpu().numpy()\n",
        "\n",
        "# To keep t-SNE stable and fast, optionally subsample if huge\n",
        "MAX_TSNE = 5000\n",
        "if feats.shape[0] > MAX_TSNE:\n",
        "    rng = np.random.default_rng(42)\n",
        "    idx = rng.choice(feats.shape[0], size=MAX_TSNE, replace=False)\n",
        "    feats_vis = feats[idx]\n",
        "    labs_vis  = labs[idx]\n",
        "else:\n",
        "    feats_vis, labs_vis = feats, labs\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\", init=\"pca\", random_state=42)\n",
        "emb  = tsne.fit_transform(feats_vis)\n",
        "\n",
        "plt.figure(figsize=(7, 6), dpi=400)\n",
        "sc = plt.scatter(emb[:,0], emb[:,1], c=labs_vis, s=5, alpha=0.8)\n",
        "plt.title(\"t-SNE of Learned Representations (penultimate features)\")\n",
        "plt.xlabel(\"t-SNE dim 1\"); plt.ylabel(\"t-SNE dim 2\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUT_DIR}/tsne_features.png\", bbox_inches=\"tight\", facecolor=\"white\")\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved:\",\n",
        "      f\"{OUT_DIR}/per_class_accuracy_bar.png,\",\n",
        "      f\"{OUT_DIR}/roc_curves_micro_macro_onevsrest.png,\",\n",
        "      f\"{OUT_DIR}/tsne_features.png\")\n",
        "# ========= END ADD =========\n",
        "\n",
        "\n",
        "\n",
        "import json, os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# class names from label_map\n",
        "class_names = None\n",
        "if os.path.exists(\"label_map.json\"):\n",
        "    with open(\"label_map.json\") as f:\n",
        "        lm = json.load(f)\n",
        "    inv = {v: k for k, v in lm.items()}\n",
        "    class_names = [inv[i] for i in range(len(inv))]\n",
        "else:\n",
        "    n_classes = cm.shape[0]\n",
        "    class_names = [str(i) for i in range(n_classes)]\n",
        "\n",
        "def _annotated_confmat(M, title, outfile, fmt=\"d\", normalize=False):\n",
        "    \"\"\"Save a confusion matrix heatmap (PNG, high DPI) with a white background.\"\"\"\n",
        "    fig_w = max(6, min(0.35 * M.shape[1], 20))\n",
        "    fig_h = max(5, min(0.35 * M.shape[0], 20))\n",
        "    fig = plt.figure(figsize=(fig_w, fig_h), dpi=600, facecolor=\"white\")\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_facecolor(\"white\")\n",
        "\n",
        "    # Normalization if requested\n",
        "    if normalize:\n",
        "        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
        "            row_sums = M.sum(axis=1, keepdims=True)\n",
        "            M_plot = np.divide(M, row_sums, out=np.zeros_like(M, dtype=float), where=row_sums!=0) * 100.0\n",
        "    else:\n",
        "        M_plot = M\n",
        "\n",
        "    # visually clean colormap (e.g., viridis, cividis, or blues)\n",
        "    im = ax.imshow(M_plot, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    cbar.ax.set_ylabel(\"%\" if normalize else \"Count\", rotation=90, va=\"center\", fontsize=8)\n",
        "\n",
        "    ax.set_title(title, pad=12, fontsize=10)\n",
        "    ax.set_xlabel(\"Predicted\", fontsize=9)\n",
        "    ax.set_ylabel(\"True\", fontsize=9)\n",
        "\n",
        "    tick_idx = np.arange(M.shape[0])\n",
        "    max_ticks = 50\n",
        "    if len(class_names) <= max_ticks:\n",
        "        ax.set_xticks(tick_idx)\n",
        "        ax.set_xticklabels(class_names, rotation=90, fontsize=7)\n",
        "        ax.set_yticks(tick_idx)\n",
        "        ax.set_yticklabels(class_names, fontsize=7)\n",
        "    else:\n",
        "        ax.set_xticks(tick_idx)\n",
        "        ax.set_xticklabels(tick_idx, rotation=90, fontsize=7)\n",
        "        ax.set_yticks(tick_idx)\n",
        "        ax.set_yticklabels(tick_idx, fontsize=7)\n",
        "\n",
        "    # Annotate cells if matrix is not too large\n",
        "    if M.shape[0] <= 50:\n",
        "        display = M_plot if normalize else M.astype(int)\n",
        "        fmt_str = \"{:.1f}\" if normalize else \"{:d}\"\n",
        "        thresh = np.nanmax(M_plot) * 0.5 if np.isfinite(np.nanmax(M_plot)) else 0.0\n",
        "        for i in range(M.shape[0]):\n",
        "            for j in range(M.shape[1]):\n",
        "                val = display[i, j]\n",
        "                color = \"white\" if (np.isfinite(M_plot[i, j]) and M_plot[i, j] > thresh) else \"black\"\n",
        "                ax.text(j, i, fmt_str.format(val),\n",
        "                        ha=\"center\", va=\"center\", color=color, fontsize=6)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(outfile, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "    plt.close(fig)\n",
        "\n",
        "# Save both versions (white background)\n",
        "_annotated_confmat(cm, \"Confusion Matrix (Counts)\",\n",
        "                   \"/content/drive/MyDrive/code_data25/Basu/save_results/confusion_matrix_counts.png\",\n",
        "                   fmt=\"d\", normalize=False)\n",
        "_annotated_confmat(cm, \"Confusion Matrix (Row-Normalized %)\",\n",
        "                   \"/content/drive/MyDrive/code_data25/Basu/save_results/confusion_matrix_normalized.png\",\n",
        "                   fmt=\".1f\", normalize=True)\n",
        "\n",
        "print(\"Saved white-background PNGs: confusion_matrix_counts.png, confusion_matrix_normalized.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl1HlU949csO",
        "outputId": "cd6902af-713d-48df-860d-a9555788e7b7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded weights. Missing keys: [] | Unexpected keys: []\n",
            "Saved: test_accuracy.txt, classification_report.txt, confusion_matrix.txt, predictions.csv, per_class_accuracy.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-362078712.py:54: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  per_class_acc = df.groupby(\"y_true\").apply(lambda g: float((g.y_true == g.y_pred).mean()))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"device\": \"cuda\",\n",
            "  \"batch_size\": 1,\n",
            "  \"runs\": 200,\n",
            "  \"per_sample_time_s_mean\": 0.0001530160018801689,\n",
            "  \"per_sample_time_s_median\": 0.0001505279988050461,\n",
            "  \"per_sample_time_s_p90\": 0.000158720001578331,\n",
            "  \"per_sample_time_s_p95\": 0.00017107839807867998,\n",
            "  \"per_sample_time_s_min\": 0.00014745600521564483,\n",
            "  \"throughput_samples_per_s_mean\": 6535.2642057863195,\n",
            "  \"params_total\": 8404510,\n",
            "  \"flops_per_sample\": 16807936,\n",
            "  \"gflops_per_sample\": 0.016807936\n",
            "}\n",
            "Saved /content/drive/MyDrive/code_data25/Basu/save_results/inference_profile.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-362078712.py:158: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  per_class_acc = df_eval.groupby(\"y_true\").apply(lambda g: float((g.y_true == g.y_pred).mean()))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/code_data25/Basu/save_results/per_class_accuracy_bar.png, /content/drive/MyDrive/code_data25/Basu/save_results/roc_curves_micro_macro_onevsrest.png, /content/drive/MyDrive/code_data25/Basu/save_results/tsne_features.png\n",
            "Saved white-background PNGs: confusion_matrix_counts.png, confusion_matrix_normalized.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qs2ub5xNE1g6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}